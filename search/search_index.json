{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","text_tokens":["md","server","commands","exit","project",".","org","a","...","h","-","live"," ","dir","welcome","/","homepage","files","visit","and","markdown","to","print","file","pages","message",",","name","index","images","create","start","new","serve","mkdocs","full","[","documentation","layout","for","other","docs","help","build","]","#","configuration","the","site","yml","reloading"],"title":"Index","title_tokens":["index"]},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","text_tokens":["mkdocs","visit",".","org","full","documentation","for"," "],"title":"Welcome to MkDocs","title_tokens":["to","welcome","mkdocs"," "]},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","text_tokens":["server","exit","project",".","a","h","live","-"," ","dir","and","print","message","name","create","start","new","serve","mkdocs","[","documentation","docs","help","build","]","the","site","reloading"],"title":"Commands","title_tokens":["commands"]},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","text_tokens":["md",".","...","/"," ","files","homepage","and","markdown","file","pages",",","index","images","mkdocs","documentation","other","docs","#","configuration","the","yml"],"title":"Project layout","title_tokens":["project","layout"," "]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/","text":"无监督学习是指在没有目标的情况下寻找输入数据的有趣变换，其目的在于数据可视化、 数据压缩、数据去噪或更好地理解数据中的相关性。无监督学习是数据分析的必备技能，在解 决监督学习问题之前，为了更好地了解数据集，它通常是一个必要步骤。降维(dimensionality reduction)和聚类(clustering)都是众所周知的无监督学习方法。","text_tokens":["数据分析","其","下","理解","有趣","去","了解",")","，","解","目标","学习","在","是","(","dimensionality"," ","众所周知","压缩","没有","相关性","在于","指","情况","地","目的","输入","数据","中","相关","更好","分析","可视化","一个","变换","、","问题","集","通常","都","周知","clustering","技能","它","噪","监督","必要","之前","方法","寻找","可视","。","必备","聚类","步骤","数据压缩","降维","无","的","为了","和","或","决","reduction"],"title":"机器学习无监督学习","title_tokens":["无","监督","学习","机器"]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/","text":"自监督学习是监督学习的一个特例，它与众不同，值得单独归为一类。自监督学习是没有 人工标注的标签的监督学习，你可以将它看作没有人类参与的监督学习。标签仍然存在(因为 总要有什么东西来监督学习过程)，但它们是从输入数据中生成的，通常是使用启发式算法生 成的。 举个例子，自编码器(autoencoder)是有名的自监督学习的例子，其生成的目标就是未经 修改的输入。同样，给定视频中过去的帧来预测下一帧，或者给定文本中前面的词来预测下一个词， 都是自监督学习的例子[这两个例子也属于时序监督学习(temporally supervised learning)，即用 未来的输入数据作为监督]。注意，监督学习、自监督学习和无监督学习之间的区别有时很模糊， 这三个类别更像是没有明确界限的连续体。自监督学习可以被重新解释为监督学习或无监督学 习，这取决于你关注的是学习机制还是应用场景。","text_tokens":["发式","码器","时序","未经","，","预测","目标","词","也","中","一个","编码器","帧","编码","举个","可以","单独","autoencoder","]","重新","过去","什么","无","和","人类","未来","自","temporally","取决","特例","类别","学习","是","总要","(","体","解释"," ","因为","都","supervised","使用","人工","视频","更","与众不同","取决于","[","场景","就是","给定","值得","习","参与","其","模糊","两个","但","或者","机制","启发","被","属于","即用","算法","它们","learning","有时","来","看作","像是","归为","前面","生成","有","界限","。","为","存在","是从","词来","这","注意","作为","或","应用","区别","下","一帧","过程","学","同样","生","没有","不同","输入","你","数据","成","还是","、","通常","标签","文本","例子","将","之间","它","监督","一类","东西","有名","启发式","三个","连续","关注","修改","的","明确","仍然","标注","很",")"],"title":"机器学习 自监督学习","title_tokens":["机器","学习","监督","自"," "]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99-Apriori%E7%AE%97%E6%B3%95/","text":"关联规则学习方法能够提取出对数据中的变量之间的关系的最佳解释。比如说一家超市的销售数据中存在规则 {洋葱，土豆}=> {汉堡}，那说明当一位客户同时购买了洋葱和土豆的时候，他很有可能还会购买汉堡肉。 例子： Apriori 算法（Apriori algorithm） Eclat 算法（Eclat algorithm） FP-growth Apriori算法是一种挖掘关联规则的算法，用于挖掘其内含的、未知的却又实际存在的数据关系，其核心是基于两阶段频集思想的递推算法 。 Apriori算法分为两个阶段： 寻找频繁项集 由频繁项集找关联规则 算法缺点： 在每一步产生侯选项目集时循环产生的组合过多，没有排除不应该参与组合的元素； 每次计算项集的支持度时，都对数据库中 的全部记录进行了一遍扫描比较，需要很大的I/O负载。","text_tokens":["进行",">","，","一位","当","时候","同时","/","内含","支持","关联","中","未知","提取","：","元素","排除","扫描","o","可能","组合","一步","了","关系","和","eclat","出对","}","缺点","）","产生","购买","algorithm","不","集时","学习","是","递推","却","解释","循环"," ","计算","比较","据库","度时","都","每次","一遍","fp","实际","（","过多","又","变量","土豆","项集","其","参与","两个","分为","i","在","说明","两","还会","-","思想","很","算法","能够","找","比如","=","最佳","有","方法","频繁","项目","。","负载","存在","肉","比如说","超市","那","挖掘","需要","数据库","全部","growth","{","一种","应该","客户","汉堡","；","没有","记录","数据","基于","规则","、","频集","由","他","例子","用于","之间","每","核心","洋葱","寻找","销售","对","一家","侯选","很大","的","阶段","规则学习","apriori"],"title":"关联规则 Apriori算法","title_tokens":["算法","关联","规则","apriori"," "]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-autoencoder/","text":"autoencoder异常检测 http://sofasofa.io/tutorials/anomaly_detection/","text_tokens":["io","http","autoencoder",":","/","detection","检测","anomaly",".","sofasofa","_","tutorials","异常"," "],"title":"异常检测 autoencoder","title_tokens":["检测","autoencoder","异常"," "]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-iTree/","text":"南大周志华老师在2010年提出一个异常检测算法Isolation Forest，在工业界很实用，算法效果好，时间效率高，能有效处理高维数据和海量数据，这里对这个算法进行简要总结。 iTree 提到森林，自然少不了树，毕竟森林都是由树构成的，看Isolation Forest（简称iForest）前，我们先来看看Isolation Tree（简称iTree）是怎么构成的，iTree是一种随机二叉树，每个节点要么有两个女儿，要么就是叶子节点，一个孩子都没有。给定一堆数据集D，这里D的所有属性都是连续型的变量，iTree的构成过程如下： 随机选择一个属性Attr； 随机选择该属性的一个值Value； 根据Attr对每条记录进行分类，把Attr小于Value的记录放在左女儿，把大于等于Value的记录放在右孩子； 然后递归的构造左女儿和右女儿，直到满足以下条件： 传入的数据集只有一条记录或者多条一样的记录； 树的高度达到了限定高度； iTree构建好了后，就可以对数据进行预测啦，预测的过程就是把测试记录在iTree上走一下，看测试记录落在哪个叶子节点。iTree能有效检测异常的假设是：异常点一般都是非常稀有的，在iTree中会很快被划分到叶子节点，因此可以用叶子节点到根节点的路径h(x)长度来判断一条记录x是否是异常点；对于一个包含n条记录的数据集，其构造的树的高度最小值为log(n)，最大值为n-1，论文提到说用log(n)和n-1归一化不能保证有界和不方便比较，用一个稍微复杂一点的归一化公式：$$s(x,n) = 2^{(-\\frac{h(x)}{c(n)})}$$,$$ c(n) = 2H(n − 1) − (2(n − 1)/n), 其中 H(k) = ln(k) + \\xi，\\xi为欧拉常数$$ $s(x,n)$就是记录x在由n个样本的训练数据构成的iTree的异常指数，$s(x,n)$取值范围为[0,1]，越接近1表示是异常点的可能性高，越接近0表示是正常点的可能性比较高，如果大部分的训练样本的s(x,n)都接近于0.5，说明整个数据集都没有明显的异常值。 随机选属性，随机选属性值，一棵树这么随便搞肯定是不靠谱，但是把多棵树结合起来就变强大了； iForest iTree搞明白了，我们现在来看看iForest是怎么构造的，给定一个包含n条记录的数据集D，如何构造一个iForest。iForest和Random Forest的方法有些类似，都是随机采样一一部分数据集去构造每一棵树，保证不同树之间的差异性，不过iForest与RF不同，采样的数据量$Psi$不需要等于n，可以远远小于n，论文中提到采样大小超过256效果就提升不大了，明确越大还会造成计算时间的上的浪费，为什么不像其他算法一样，数据越多效果越好呢，可以看看下面这两个个图， 左边是元素数据，右边是采样了数据，蓝色是正常样本，红色是异常样本。可以看到，在采样之前，正常样本和异常样本出现重叠，因此很难分开，但我们采样之和，异常样本和正常样本可以明显的分开。 除了限制采样大小以外，还要给每棵iTree设置最大高度$l=ceiling(log_2^\\Psi)$，这是因为异常数据记录都比较少，其路径长度也比较低，而我们也只需要把正常记录和异常记录区分开来，因此只需要关心低于平均高度的部分就好，这样算法效率更高，不过这样调整了后，后面可以看到计算$h(x)$需要一点点改进，先看iForest的伪代码： IForest构造好后，对测试进行预测时，需要进行综合每棵树的结果，于是$$s(x,n) = 2^{(-\\frac{E(h(x))}{c(n)})}$$ $E(h(x))$表示记录x在每棵树的高度均值，另外h(x)计算需要改进，在生成叶节点时，算法记录了叶节点包含的记录数量，这时候要用这个数量$Size$估计一下平均高度，h(x)的计算方法如下： 处理高维数据 在处理高维数据时，可以对算法进行改进，采样之后并不是把所有的属性都用上，而是用峰度系数Kurtosis挑选一些有价值的属性，再进行iTree的构造，这跟随机森林就更像了，随机选记录，再随机选属性。 只使用正常样本 这个算法本质上是一个无监督学习，不需要数据的类标，有时候异常数据太少了，少到我们只舍得拿这几个异常样本进行测试，不能进行训练，论文提到只用正常样本构建IForest也是可行的，效果有降低，但也还不错，并可以通过适当调整采样大小来提高效果。 全文完，转载请注明出处：http://www.cnblogs.com/fengfenggirl/p/iForest.html （1） iForest具有线性时间复杂度。因为是ensemble的方法，所以可以用在含有海量数据的数据集上面。通常树的数量越多，算法越稳定。由于每棵树都是互相独立生成的，因此可以部署在大规模分布式系统上来加速运算。 （2） iForest不适用于特别高维的数据。由于每次切数据空间都是随机选取一个维度，建完树后仍然有大量的维度信息没有被使用，导致算法可靠性降低。高维空间还可能存在大量噪音维度或无关维度（irrelevant attributes），影响树的构建。对这类数据，建议使用子空间异常检测（Subspace Anomaly Detection）技术。此外，切割平面默认是axis-parallel的，也可以随机生成各种角度的切割平面，详见“On Detecting Clustered Anomalies Using SCiForest”。 （3） iForest仅对Global Anomaly 敏感，即全局稀疏点敏感，不擅长处理局部的相对稀疏点 （Local Anomaly）。目前已有改进方法发表于PAKDD，详见“Improving iForest with Relative Mass”。 （4） iForest推动了重心估计（Mass Estimation）理论发展，目前在分类聚类和异常检测中都取得显著效果，发表于各大顶级数据挖掘会议和期刊（如SIGKDD，ICDM，ECML）。 实现： https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html 注意：提供的所有参数都要一一进行分析 实现： 主要命令 http://help.aliyun-inc.com/internaldoc/detail/34584.html?spm=a2c1f.8259796.2.198.791296d5Wo21KP iforest训练命令 https://pai.dw.alibaba-inc.com/component/detail/453?projectId=20212&spm=a2c3x.12342929.0.0.661d4a9bFxmP7T 预测命令 https://pai.dw.alibaba-inc.com/component/detail/227?projectId=20212&spm=a2c3x.12342929.0.0.661d4a9bFxmP7T","text_tokens":["预测","时候","一些","也","其中","：","但是","满足","仅","取值","不像","一点","help","另外","aliyun","稍微","大小","复杂","大于","）","价值","不大","cnblogs","啦","孩子","蓝色","对于","根","isolation","c","不了","学习","axis","呢"," ","每个","detection","一棵","顶级","256","大规","后","有时候","差异","小于","异性","使用","更","选取","期刊","接近","而是","部署","每棵","太少","itree","稀疏","效率高","浪费","227","随机","时","a2c3x.12342929","在","非常","算法","1","路径","e","有时","信息","特别","大部","集","这样","判断","远远","有效",",","关心","生成","平面","女儿","其他","有","而","d","影响","要么","。","年","先看","综合","是否是","最大值","前","个","挖掘","跟","一","value","再","过程","发表","维度","于是","psi","估计","转载","拿","适当","左边","一般","肯定","数据","rf","最大","html","就","是否","如何","构建","总结","之间","分类","挑选","样本","少不了","老师","对","只有","敏感","的","learn","参数","improving",")","整个","表示","一点点","进行","以下","并","归一化","s","分布式","采样","可行","2.198","icdm","与","可能性","业界","各种","&","无关","该","规模","?","我们","均值","_","之前","欧拉","pai","了","放在","internaldoc","少","无","条件","size","变","com","上来","不","是","现在","453","sigkdd","不适","最小","稳定","属性","导致","sklearn","都","每条","开来","看","全局","这么","stable","pakdd","+","类标","每次","设置","detail","（","假设","叶子","变量","attributes","log","一一","4","长度","两个","ecml","低","测试","2","起来","这时候","造成","只用","detecting","这里","说明","调整","不能","n","系统","www","20212","布式","p","数据挖掘","l","森林","线性","2010","重叠","通过","显著","低于","on","一部","注明","为","训练样本","目前","提到","subspace","公式","isolationforest","generated","with","限制","这","frac","注意","等于","提高","自然","点点","峰度","明白","说用","提升","建议","差异性","没有","改进","区分","把","越大","连续型","通常","高","a2c1f.8259796","重心","互相","除了","每","estimation","^","集去","之后","树后","红色","2h","提供","方便","inc","上面","661d4a9bfxmp7t","检测","工业界","仍然","发展","主要","不错","大量","很难","维空间","伪","random","，","scikit",".","看到","一下","运算","搞","中","南","一个","多条","元素","怎么","于","正常","结果","构成","到","可能","会","kurtosis","好","以外","值","所以","常数","0.0","技术","集都","数据量","component","不靠","如下","实现","左","一条","ln","因为","擅长","791296d5wo21kp","比较","超过","像","分布式系统","图","越","https","训练","还要","出现","全文","是因为","达到","随便","给定","forest","走","高度","此外","只","很快","大","其","大值",":","但","一部分","”","alibaba","“","二叉树","落","不是","更高","0","可靠","来","为什么","切割","详见","越好","工业","含有","方法","能","简称","二叉","加速","聚类","大部分","如果","效果","如","需要","用","或","parallel","{","子","直到","论文","irrelevant","推动","少到","限定","明显","计算方法","不同","中会","local","独立","因此","包含","选","命令","最小值","谱","看看","切","默认","数量","x","先","下面","噪音","选择","mass","好后","海量","给","h","/","全文完","http","之","有些","出处","分析","这个","modules","周志华","可以","分开","这时","构造","局部","]","传入","−","什么","和","请","clustered","}","projectid","叶","点","相对","实用","简要","复杂度","(","多棵","计算","有界","分布","划分","毕竟","异常","还","即","处理","具有","建完","各大","理论","上","一堆","[","就是","提出","然后","显著效果","指数","ensemble","$","后面","或者","3","sciforest","被","右边","效率","-","attr","部分","时间","根据","要","iforest","已有","代码","稀有","dw","大规模","=","平均","右","k","保证","所有","哪个","由于","节点","存在","类似","fengfenggirl","global","结合","几个","树","本质","角度","anomalies","一种","34584","anomaly","org","强大","范围","spm","ceiling","舍得","；","using","tree","记录","relative","条","0.5","由","递归","xi","降低","空间","用于","可靠性","多","\\","监督","连续","取得","系数","高维","这类","一棵树","明确","会议","一样","很","不过"],"title":"异常检测 iTree","title_tokens":["检测","itree","异常"," "]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-iTree/#itree","text":"提到森林，自然少不了树，毕竟森林都是由树构成的，看Isolation Forest（简称iForest）前，我们先来看看Isolation Tree（简称iTree）是怎么构成的，iTree是一种随机二叉树，每个节点要么有两个女儿，要么就是叶子节点，一个孩子都没有。给定一堆数据集D，这里D的所有属性都是连续型的变量，iTree的构成过程如下： 随机选择一个属性Attr； 随机选择该属性的一个值Value； 根据Attr对每条记录进行分类，把Attr小于Value的记录放在左女儿，把大于等于Value的记录放在右孩子； 然后递归的构造左女儿和右女儿，直到满足以下条件： 传入的数据集只有一条记录或者多条一样的记录； 树的高度达到了限定高度； iTree构建好了后，就可以对数据进行预测啦，预测的过程就是把测试记录在iTree上走一下，看测试记录落在哪个叶子节点。iTree能有效检测异常的假设是：异常点一般都是非常稀有的，在iTree中会很快被划分到叶子节点，因此可以用叶子节点到根节点的路径h(x)长度来判断一条记录x是否是异常点；对于一个包含n条记录的数据集，其构造的树的高度最小值为log(n)，最大值为n-1，论文提到说用log(n)和n-1归一化不能保证有界和不方便比较，用一个稍微复杂一点的归一化公式：$$s(x,n) = 2^{(-\\frac{h(x)}{c(n)})}$$,$$ c(n) = 2H(n − 1) − (2(n − 1)/n), 其中 H(k) = ln(k) + \\xi，\\xi为欧拉常数$$ $s(x,n)$就是记录x在由n个样本的训练数据构成的iTree的异常指数，$s(x,n)$取值范围为[0,1]，越接近1表示是异常点的可能性高，越接近0表示是正常点的可能性比较高，如果大部分的训练样本的s(x,n)都接近于0.5，说明整个数据集都没有明显的异常值。 随机选属性，随机选属性值，一棵树这么随便搞肯定是不靠谱，但是把多棵树结合起来就变强大了；","text_tokens":["选择","表示","整个","进行","以下","，","归一化","预测","s","h","/","一下","搞","一个","可能性","其中","：","多条","但是","怎么","于","该","满足","我们","正常","构成","到","可能","欧拉","可以","取值","了","一点","放在","好","构造","]","传入","−","和","值","条件","稍微","复杂","}","大于","）","常数","啦","变","集都","不","孩子","点","不靠","对于","如下","根","isolation","c","是","不了","("," ","左","一条","ln","多棵","每个","有界","最小","比较","划分","属性","一棵","都","毕竟","后","每条","小于","异常","看","越","这么","训练","+","上","一堆","[","接近","就是","（","itree","达到","假设","给定","随便","叶子","然后","forest","走","高度","指数","变量","很快","log","其","长度","两个","随机","大值","$","或者","测试","2","起来","被","在","这里","说明","二叉树","落","attr","n","-","非常","不能","1","路径","0","部分","根据","来","大部","iforest","集","稀有","森林","判断","有效",",","=","女儿","右","有","k","保证","d","能","简称","二叉","要么","。","所有","节点","哪个","为","大部分","提到","是否是","如果","公式","训练样本","最大值","结合","前","个","树","用","frac","等于","{","直到","value","自然","一种","论文","过程","限定","强大","范围","说用","明显","；","没有","tree","记录","把","条","一般","0.5","数据","连续型","中会","肯定","最大","由","因此","递归","包含","xi","选","就","是否","高","构建","最小值","谱","看看","分类","^","\\","样本","少不了","连续","2h","对","方便","只有","x","先","的","检测","一棵树","一样",")"],"title":"iTree","title_tokens":["itree"]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-iTree/#iforest","text":"iTree搞明白了，我们现在来看看iForest是怎么构造的，给定一个包含n条记录的数据集D，如何构造一个iForest。iForest和Random Forest的方法有些类似，都是随机采样一一部分数据集去构造每一棵树，保证不同树之间的差异性，不过iForest与RF不同，采样的数据量$Psi$不需要等于n，可以远远小于n，论文中提到采样大小超过256效果就提升不大了，明确越大还会造成计算时间的上的浪费，为什么不像其他算法一样，数据越多效果越好呢，可以看看下面这两个个图， 左边是元素数据，右边是采样了数据，蓝色是正常样本，红色是异常样本。可以看到，在采样之前，正常样本和异常样本出现重叠，因此很难分开，但我们采样之和，异常样本和正常样本可以明显的分开。 除了限制采样大小以外，还要给每棵iTree设置最大高度$l=ceiling(log_2^\\Psi)$，这是因为异常数据记录都比较少，其路径长度也比较低，而我们也只需要把正常记录和异常记录区分开来，因此只需要关心低于平均高度的部分就好，这样算法效率更高，不过这样调整了后，后面可以看到计算$h(x)$需要一点点改进，先看iForest的伪代码： IForest构造好后，对测试进行预测时，需要进行综合每棵树的结果，于是$$s(x,n) = 2^{(-\\frac{E(h(x))}{c(n)})}$$ $E(h(x))$表示记录x在每棵树的高度均值，另外h(x)计算需要改进，在生成叶节点时，算法记录了叶节点包含的记录数量，这时候要用这个数量$Size$估计一下平均高度，h(x)的计算方法如下：","text_tokens":["一点点","表示","进行","好后","伪","random","，","预测","时候","看到","s","给","很难","h","采样","一下","之","有些","与","搞","也","中","一个","：","这个","元素","怎么","我们","正常","结果","均值","_","会","之前","可以","分开","不像","了","一点","好","这时","构造","以外","另外","少","什么","和","大小","}","size","不大","数据量","不","叶","蓝色","如下","c","是","现在","呢","("," ","计算","因为","一棵","比较","超过","都","256","后","差异","小于","还","图","异常","越","异性","开来","还要","出现","上","设置","是因为","每棵","itree","给定","forest","高度","只","浪费","log","其","长度","两个","随机","时","$","一部分","但",")","低","2","后面","测试","这时候","造成","右边","在","调整","效率","-","n","更高","算法","路径","部分","e","时间","要","来","iforest","集","l","为什么","这样","代码","越好","远远","重叠",",","关心","生成","=","低于","平均","其他","方法","一部","保证","d","而","。","节点","先看","类似","提到","综合","效果","个","树","需要","限制","这","frac","用","等于","{","一","论文","点点","明白","明显","ceiling","提升","于是","psi","估计","差异性","改进","计算方法","记录","区分","把","条","不同","越大","数据","rf","左边","最大","因此","包含","就","如何","之间","除了","每","看看","多","集去","^","\\","样本","红色","对","数量","x","下面","的","一棵树","明确","一样","不过"],"title":"iForest","title_tokens":["iforest"]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-iTree/#_1","text":"在处理高维数据时，可以对算法进行改进，采样之后并不是把所有的属性都用上，而是用峰度系数Kurtosis挑选一些有价值的属性，再进行iTree的构造，这跟随机森林就更像了，随机选记录，再随机选属性。","text_tokens":["跟","价值","时","进行","再","随机","并","，","峰度","在","一些","采样","不是","算法","改进","记录","把","数据","属性","都","像","这","森林","选","处理","就","更","之后","挑选","有","上","可以","而是","所有","系数","kurtosis","itree","构造","对","了","。","高维","的","用"],"title":"处理高维数据","title_tokens":["处理","高维","数据"]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-iTree/#_2","text":"这个算法本质上是一个无监督学习，不需要数据的类标，有时候异常数据太少了，少到我们只舍得拿这几个异常样本进行测试，不能进行训练，论文提到只用正常样本构建IForest也是可行的，效果有降低，但也还不错，并可以通过适当调整采样大小来提高效果。 全文完，转载请注明出处：http://www.cnblogs.com/fengfenggirl/p/iForest.html （1） iForest具有线性时间复杂度。因为是ensemble的方法，所以可以用在含有海量数据的数据集上面。通常树的数量越多，算法越稳定。由于每棵树都是互相独立生成的，因此可以部署在大规模分布式系统上来加速运算。 （2） iForest不适用于特别高维的数据。由于每次切数据空间都是随机选取一个维度，建完树后仍然有大量的维度信息没有被使用，导致算法可靠性降低。高维空间还可能存在大量噪音维度或无关维度（irrelevant attributes），影响树的构建。对这类数据，建议使用子空间异常检测（Subspace Anomaly Detection）技术。此外，切割平面默认是axis-parallel的，也可以随机生成各种角度的切割平面，详见“On Detecting Clustered Anomalies Using SCiForest”。 （3） iForest仅对Global Anomaly 敏感，即全局稀疏点敏感，不擅长处理局部的相对稀疏点 （Local Anomaly）。目前已有改进方法发表于PAKDD，详见“Improving iForest with Relative Mass”。 （4） iForest推动了重心估计（Mass Estimation）理论发展，目前在分类聚类和异常检测中都取得显著效果，发表于各大顶级数据挖掘会议和期刊（如SIGKDD，ICDM，ECML）。 实现： https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html 注意：提供的所有参数都要一一进行分析","text_tokens":["维空间","进行","mass","并","海量","，","scikit",".","时候","分布式","/","采样","可行","全文完","http","icdm","运算","出处","也","中","分析","一个","各种","：","这个","无关","于","modules","仅","规模","我们","正常","可能","可以","了","局部","无","和","请","clustered","大小","复杂","）","所以","cnblogs","com","上来","技术","不","点","相对","学习","是","复杂度","axis","实现"," ","因为","分布","sigkdd","不适","擅长","detection","稳定","sklearn","导致","都","顶级","大规","有时候","分布式系统","异常","还","即","越","全局","处理","使用","具有","建完","训练","pakdd","各大","类标","选取","每次","https","stable","理论","全文","上","期刊","部署","噪音","太少","（","每棵","稀疏","显著效果","此外","只","attributes","一一","4","ensemble","随机",":","ecml","但","”","测试","2","detecting","只用","sciforest","被","在","3","“","调整","-","不能","系统","www","算法","1","有时","信息","时间","布式","特别","可靠","要","来","iforest","p","已有","集","数据挖掘","切割","详见","线性","通过","生成","大规模","平面","含有","显著","大量","on","有","方法","影响","。","注明","由于","加速","存在","目前","聚类","提到","fengfenggirl","subspace","global","isolationforest","几个","效果","generated","树","with","挖掘","需要","如","这","用","注意","本质","或","parallel","子","角度","anomalies","论文","提高","irrelevant","推动","发表","少到","anomaly","org","维度","舍得","转载","建议","估计","所有","没有","using","改进","拿","适当","relative","数据","html","local","通常","独立","因此","降低","高","空间","重心","构建","互相","用于","可靠性","estimation","多","分类","树后","监督","样本","切","默认","取得","提供","对","数量","高维","上面","敏感","的","这类","检测","learn","仍然","会议","参数","发展","improving","不错"],"title":"只使用正常样本","title_tokens":["正常","只","使用","样本"]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-iTree/#_3","text":"主要命令 http://help.aliyun-inc.com/internaldoc/detail/34584.html?spm=a2c1f.8259796.2.198.791296d5Wo21KP iforest训练命令 https://pai.dw.alibaba-inc.com/component/detail/453?projectId=20212&spm=a2c3x.12342929.0.0.661d4a9bFxmP7T 预测命令 https://pai.dw.alibaba-inc.com/component/detail/227?projectId=20212&spm=a2c3x.12342929.0.0.661d4a9bFxmP7T","text_tokens":["projectid","0.0",":","com","a2c3x.12342929","alibaba","34584","component",".","预测","spm","-","/"," ","453","2.198","20212","http","791296d5wo21kp","html","&","iforest","命令","a2c1f.8259796","https","dw","=","训练","?","pai","detail","help","inc","internaldoc","aliyun","661d4a9bfxmp7t","主要","227"],"title":"实现：","title_tokens":["实现","："]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/AADE/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-AADE-CompactDescription/","text":"In this section, we first describe the compact description formalism to describe a group of instances. Subsequently, we propose algorithms for selecting diverse instances for querying and to generate succinct interpretable rules using compact description. Our goal is to select a small subset of the most relevant and “compact” (by volume) subspaces which together contain all the instances in a group that we want to describe. We treat this problem as a specific instance of the set covering problem. This approach can be potentially interpreted as a form of non-parametric clustering. Applications of Compact Description. Compact descriptions have multiple uses including: Discovery of diverse classes of anomalies very quickly by querying instances from different subspaces of the description. Interpretable Explanations from Subspaces. Descriptions (of anomalies) should be simple . Subspaces in the case of tree-based models are defined by ranges of feature values which can be translated to predicate rules. Descriptions should be precise , i.e., they should include few nominals (false-positives). The absence of this property would result in descriptions with high recall but low precision.","text_tokens":["defined","small","based","absence",".","including","contain","compact","selecting","relevant","covering","approach","classes","generate","models","high","feature","potentially","few","is","instances","ranges","formalism","want","of","(","volume","all"," ","translated","but","very","applications","from","property","as","multiple","precision","include","for","description","non","subset","succinct","the","treat","algorithms","quickly","querying","our",":","”","i","values","rules","most","in","“","should","-","first","e","we","have","select","explanations","to","they","describe","case","section",",","nominals","problem","would","subspaces","form","instance","set","are","with","subsequently","that","interpretable","anomalies","precise","a","false","specific","using","tree","low","be","predicate","recall","and","goal","by","descriptions","parametric","diverse","clustering","can","simple","interpreted","different","result","positives","uses","group","propose","together","this","discovery","which",")"],"title":"异常检测 AADE CompactDescription","title_tokens":["aade","检测","compactdescription","异常"," "]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/AADE/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-AADE-Diversity-based/","text":"It is likely that different types of instances belong to different subspaces in the original feature space.","text_tokens":["original","types","space","is","likely","different","feature","of","the",".","instances","in","to","belong","that","subspaces","it"," "],"title":"异常检测 AADE Diversity based","title_tokens":["aade","based","检测","diversity","异常"," "]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/AADE/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-AADE-ExplanationsFromSubspaces/","text":"Interpretable Explanations from Subspaces. Descriptions (of anomalies) should be simple . Subspaces in the case of tree-based models are defined by ranges of feature values which can be translated to predicate rules. Descriptions should be precise , i.e., they should include few nominals (false-positives). The absence of this property would result in descriptions with high recall but low precision.","text_tokens":["defined","based","anomalies","absence","of",".","precise","values","rules","i","in","(","should","-","false"," ","translated","but","tree","low","be","e","predicate","recall","descriptions","by","explanations","to","they","from","property","case","which","can","simple",",","nominals","precision","models","include","high","feature","would","subspaces","result","positives","few","the","are","with","this","ranges","interpretable",")"],"title":"异常检测 AADE ExplanationsFromSubspaces","title_tokens":["aade","explanationsfromsubspaces","检测","异常"," "]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/AADE/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-AADE-GLAD-FSS/","text":"GLocalized Anomaly Detection (GLAD) — which allows a human analyst to continue using anomaly detection ensembles with global behavior by learning their local relevance in different parts of the feature space via label feedback. Our GLAD algorithm automatically learns the local relevance of each ensemble member in the feature space via a neural network using the label feedback from a human analyst. since we are in an active learning setting for anomaly detection, we need to consider two important aspects: (a) Number of labeled examples is very small (possibly none), and (b) To reduce the effort of the human analyst, the algorithm needs to be primed so that the likelihood of discovering anomalies is very high from the first feedback iteration itself. Subsequently, the algorithm receives label feedback from a human analyst and determines whether the ensemble made an error (i.e., anomalous in- stances are ranked at the top and scores of anomalies are higher than scores of nominals).","text_tokens":["small","labeled",".","human","learns","an","allows","at","their","made","high","examples","feature","neural","scores","is","glad","algorithm","than","error","of","(","analyst"," ","number","detection","very","relevance","from","label","whether","aspects","automatically","ranked","for","glocalized","important","effort","continue","likelihood","receives","—","the","possibly","needs","ensemble","our",":","discovering","stances","behavior","i","in","-","first","learning","space","we","e","setting","to","active","none",",","parts","nominals","two","each","since","global","are","with","subsequently","that","so","anomalies","feedback","consider","member","anomaly","a","need","using","b","reduce","be","and","by","local","top","determines","primed","anomalous","different","higher","via","network","iteration","itself","ensembles","which",")"],"title":"异常检测 AADE GLAD FSS","title_tokens":["aade","fss","检测","glad","异常"," "]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/AADE/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-AADE-ScoreFunction/","text":"$Score(x) = w · z$ Our previous discussion then suggests that the optimal hyperplane can now be learned efficiently by greedily asking the analyst to label the most anomalous instance in each feedback iteration. Figure 10 illustrates how BAL changes the anomaly score contours across feedback iterations on the synthetic dataset using an isolation forest with 100 trees.","text_tokens":["optimal","bal","our","across","feedback","$","anomaly","greedily",".","isolation","most","in","(","synthetic","analyst"," ","using","be","contours","an","by","trees","efficiently","to","label","changes","can","anomalous","=","figure","learned","dataset","100","on","hyperplane","each","previous","iterations","how","suggests","·","iteration","10","now","x","instance","illustrates","forest","the","w","z","discussion","asking","then","with","that","score",")"],"title":"异常检测 AADE ScoreFunction","title_tokens":["aade","异常","检测","scorefunction"," "]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/AADE/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-AADE-StreamingData/","text":"In the streaming setting, we assume that the data is input to the algorithm continuously in windows of size K and is potentially unlimited.","text_tokens":["algorithm","of","unlimited",".","in","data"," ","we","setting","and","continuously","to","streaming",",","assume","k","potentially","windows","input","is","the","that","size"],"title":"异常检测 AADE StreamingData","title_tokens":["aade","检测","streamingdata","异常"," "]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/AADE/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-AADE-Tree-based/","text":"Ensemble of tree-based anomaly detectors have several attractive properties that make them an ideal candidate for active learning : They can be employed to construct large ensembles inexpensively. Treating the nodes of tree as ensemble members allows us to both focus our feedback on fine-grained subspaces as well as increase the capacity of the model in terms of separating anomalies from nominals. Since some of the tree-based models such as Isolation Forest (IFOR) (Liu et al., 2008), HS Trees (HST) (Tan, Ting, & Liu, 2011), and RS Forest (RSF) (Wu, Zhang, Fan, Edwards, & Yu, 2014) are state-of-the-art unsupervised detectors (Emmott et al., 2015; Domingues, Filippone, Michiardi, & Zouaoui, 2018), it is a significant gain if their performance can be improved with minimal label feedback . In this work, we will focus mainly on IFOR because it performed best across all datasets (Emmott et al., 2015). However, we also present results on HST and RSF wherever appli- cable. Isolation forest (Liu et al., 2008) is a state-of-the-art ensemble anomaly detector.","text_tokens":["improved","best","based","liu","attractive","separating",".","an","art","filippone","such","&","allows","trees","ideal","wu","2014","focus","ting","their","wherever","some","models","present","tan","model","2008","because","both","if","is","members","nodes","2011","edwards","emmott","of","mainly","isolation","michiardi","(","us","it"," ","all","inexpensively","significant","from","label","as","hs","yu","them","cable","fan","for","rsf","zhang","results","appli","forest","zouaoui","datasets","the","increase","ensemble","our","properties",":","2015","in","unsupervised","-","al","however","treating","learning","we","capacity","have","construct","they","to","active",",","nominals","state","on","since","subspaces","candidate","also","gain","are","rs","well","with","large",";","that","domingues","work","terms","anomalies","across","feedback","anomaly","a","hst","tree","will","be","performed","grained","and","2018","ifor","fine","can","several","employed","minimal","make","et","performance","detectors","ensembles","this","detector",")"],"title":"异常检测 AADE Tree based","title_tokens":["aade","based","tree","检测","异常"," "]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/AADE/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-AADE/","text":"we study the problem of active learning to automatically tune ensemble of anomaly detectors to maximize the number of true anomalies discovered. First, we present an important insight that explains the practical successes of AD ensembles and how ensembles are naturally suited for active learning. This insight also allows us to relate the greedy query selection strategy to uncertainty sampling, with implications for label-efficient learning. Second, we present several algorithms for active learning with tree-based AD ensembles . A novel formalism called compact description (CD) is developed to describe the discovered anomalies. We propose algorithms based on the CD formalism to improve the diversity of discovered anomalies and to generate rule sets for improved interpretability of anomalous instances. To handle streaming data setting , we present a novel data drift detection algorithm that not only detects the drift robustly, but also allows us to take corrective actions to adapt the detector in a principled manner. Third, we present a novel algorithm called GLocalized Anomaly Detection (GLAD) for active learning with generic AD ensembles and an approach to generate succinct explanations from the resulting models. GLAD allows end-users to retain the use of simple and understandable global anomaly detectors by automatically learning their local relevance to specific data instances using label feedback. Fourth, we present extensive experiments to evaluate our insights and algorithms with tree-based AD ensembles in both batch and streaming settings . Ensemble of anomaly detectors are shown to perform well in both unsupervised and active learning settings, but their characteristics leading to good performance are not well-understood Why does the average score across ensemble members perform best in most cases (Chiang & Yeh, 2015) instead of other score combination strategies (e.g., min, max, median etc.)? Why does the greedy query selection strategy for active learning almost always per- form best? # We setup a scoring function Score(x) to score data instances $Score(x) = w · z$ Our active learning framework In each iteration of active learning loop, we perform the following steps: 1) Select one or more unlabeled instances from the input dataset D according to a query selection strategy QS; 2) Query the human analyst for labels of selected instances by providing additional information in the form of interpretable rules or explanations; 3) Update the weights of the scoring function based on the aggregate set of labeled and unlabeled instances. The goal of A is to learn optimal weights for maximizing the number of true anomalies shown to the analyst. The key insights presented here are in the context of a linear combination of ensemble scores. Specifically, greedily selecting instances with the highest scores is first of all more likely to reveal anomalies (i.e., true positives), and even if the selected instance is nominal (i.e., false positive), it still helps in learning the decision boundary efficiently. This is an important insight and has significant practical implications.","text_tokens":["improved","chiang","best","based","weights","sets","labeled",".","take","min","human","improve","presented","compact","an","nominal","allows","&","unlabeled","selecting","or","approach","their","interpretability","generate","?","settings","present","models","users","other","cases","corrective","tune","scores","input","both","if","is","evaluate","suited","insights","following","cd","instances","glad","w","z","members","rule","formalism","not","optimal","query","algorithm","of","(","us","analyst"," ","but","all","it","number","according","information","detection","specifically","significant","from","relevance","scoring","setup","label","labels","shown","streaming","selection","automatically","dataset","adapt","for","important","efficient","description","glocalized","how","end","succinct","steps","aggregate","actions","batch","instead","the","algorithms","ensemble","manner","practical","developed","our","false",":","etc","selected","helps","2015","$","2","explains","3","i","rules","most","naturally","in","retain","unsupervised","-","uncertainty","one","characteristics","first","1","fourth","learning","we","e","highest","setting","still","novel","boundary","select","almost","strategies","framework","explanations","diversity","to","handle","active","describe","update","linear",",","context","=","good","successes","on","does","problem","each","d","form","strategy","perform","also","·","here","global","instance","g","decision","insight","understandable","max","are","with","well","set","positive",";","that","score","extensive","interpretable","anomalies","across","feedback","qs","even","reveal","anomaly","second","a","called","third","sampling","robustly","greedily","data","greedy","relate","implications","specific","combination","principled","using","always","tree","per","providing","yeh","and","goal","drift","by","local","efficiently","more","loop","ad","understood","several","simple","anomalous","maximizing","only","leading","maximize","resulting","median","has","average","performance","generic","detects","positives","function","detectors","true","#","iteration","ensembles","use","x","propose","why","learn","study","experiments","discovered","key","this","likely","additional","detector",")"],"title":"异常检测 AADE","title_tokens":["检测","aade","异常"," "]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/AADE/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-AADE/#_1","text":"Ensemble of anomaly detectors are shown to perform well in both unsupervised and active learning settings, but their characteristics leading to good performance are not well-understood Why does the average score across ensemble members perform best in most cases (Chiang & Yeh, 2015) instead of other score combination strategies (e.g., min, max, median etc.)? Why does the greedy query selection strategy for active learning almost always per- form best? # We setup a scoring function Score(x) to score data instances $Score(x) = w · z$","text_tokens":["chiang","best",".","min","&","their","?","settings","other","cases","both","w","instances","z","members","not","query","of","("," ","but","scoring","setup","shown","selection","for","instead","the","ensemble","etc","2015","$","most","in","unsupervised","-","characteristics","learning","e","we","almost","strategies","to","active",",","=","good","does","form","strategy","perform","·","g","max","are","well","score","across","anomaly","a","data","greedy","combination","always","per","yeh","and","understood","leading","median","average","performance","function","detectors","#","x","why",")"],"title":"","title_tokens":[]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/AADE/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-AADE/#_2","text":"Our active learning framework In each iteration of active learning loop, we perform the following steps: 1) Select one or more unlabeled instances from the input dataset D according to a query selection strategy QS; 2) Query the human analyst for labels of selected instances by providing additional information in the form of interpretable rules or explanations; 3) Update the weights of the scoring function based on the aggregate set of labeled and unlabeled instances. The goal of A is to learn optimal weights for maximizing the number of true anomalies shown to the analyst.","text_tokens":["optimal","based","our","query",":","weights","selected","labeled","anomalies","qs","2","of","3","a",".","rules","in","human","analyst"," ","one","1","learning","we","providing","according","information","number","and","goal","select","framework","by","unlabeled","explanations","to","or","active","more","from","loop","update","labels","scoring","shown",",","selection","maximizing","dataset","on","each","d","for","form","strategy","function","perform","steps","aggregate","input","true","iteration","is","learn","the","following","set","instances",";","additional","interpretable",")"],"title":"","title_tokens":[]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/AADE/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-AADE/#_3","text":"The key insights presented here are in the context of a linear combination of ensemble scores. Specifically, greedily selecting instances with the highest scores is first of all more likely to reveal anomalies (i.e., true positives), and even if the selected instance is nominal (i.e., false positive), it still helps in learning the decision boundary efficiently. This is an important insight and has significant practical implications.","text_tokens":["ensemble","practical","helps","anomalies","selected","even","reveal","of",".","a","greedily","i","in","(","it","all","combination"," ","false","implications","first","presented","still","e","learning","boundary","highest","an","and","nominal","specifically","linear","selecting","more","to","efficiently","significant",",","context","has","important","positives","scores","true","if","here","is","instance","likely","decision","positive","insight","the","insights","are","key","instances","with","this",")"],"title":"","title_tokens":[]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/AADE/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-AADE/#_4","text":"","text_tokens":[],"title":"","title_tokens":[]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/AADE/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-ActiveLearning/","text":"Active learning corresponds to the setup where the learning algorithm can selectively query a human analyst for labels of input instances to improve its prediction accuracy.","text_tokens":["query","corresponds","algorithm","of",".","a","human","improve","analyst"," ","learning","to","active","setup","labels","can","prediction","for","accuracy","input","its","where","selectively","the","instances"],"title":"异常检测 ActiveLearning","title_tokens":["检测","activelearning","异常"," "]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/AADE/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-%E4%BB%BB%E5%8A%A1-%E6%8C%91%E6%88%98/","text":"However, this process can be laborious for the human analyst when the number of false-positives is very high. Therefore, in many real-world AD applications including computer security and fraud prevention, the anomaly detector must be configurable by the human analyst to minimize the effort on false positives. One important way to configure the detector is by providing true labels (nominal or anomaly) for a few instances. Anomaly detection poses severe challenges that are not seen in traditional learning prob- lems. First, anomalies are significantly fewer in number than nominal instances. Second, unlike classification problems, no hard decision boundary exists to separate anomalies and nominals. Instead, anomaly detection algorithms train models to compute scores for all instances, and report instances which receive the highest scores as anomalies . The candi- date anomalies in the form of top-ranked data instances are analyzed by a human analyst to identify the true anomalies . Since most of the AD algorithms (Chandola, Banerjee, & Kumar, 2009) only report technical outliers (i.e., data instances which do not fit a nor- mal model as anomalies), the candidate set of anomalies may contain many false-positives, which will significantly increase the effort of human analyst in discovering true anomalies.","text_tokens":[".","human","kumar","including","contain","hard","fraud","classification","nominal","&","or","analyzed","no","train","models","high","model","may","few","scores","fit","outliers","is","identify","instances","poses","report","chandola","not","when","challenges","than","of","(","analyst"," ","all","therefore","number","detection","very","applications","severe","problems","labels","as","lems","nor","fewer","ranked","for","important","seen","date","effort","instead","the","banerjee","algorithms","unlike","increase","world","discovering","i","most","in","-","however","one","first","learning","e","boundary","highest","configure","to","technical","process","prevention",",","nominals","on","computer","since","many","form","candidate","receive","decision","set","real","are","that","minimize","compute","anomalies","laborious","anomaly","must","a","second","candi","data","2009","false","significantly","be","providing","do","will","and","prob","by","top","exists","ad","can","way","separate","only","positives","true","mal","configurable","this","traditional","security","detector","which",")"],"title":"异常检测 任务 挑战","title_tokens":["挑战","任务","检测","异常"," "]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/AADE/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-%E4%BB%BB%E5%8A%A1-%E7%BC%BA%E7%82%B9/","text":"Prior work on anomaly detection has four main shortcomings as explained in the related work section. First, many algorithms are unsupervised in nature and do not provide a way to configure the anomaly detector by the human analyst to minimize the effort on false-positives. There is very little work on principled active learning algorithms. Second, algorithmic work on enhancing the diversity of discovered anomalies is lacking (G ̈ornitz, Kloft, Rieck, & Brefeld, 2013). Third, most algorithms are designed to handle batch data well, but there are few principled algorithms to handle streaming data setting. Fourth, there is little to no work on interpretability and explainability in the context of anomaly detection tasks (Macha & Akoglu, 2018).","text_tokens":["provide","brefeld",".","kloft","human","algorithmic","&","no","explained","interpretability","few","is","little","not","of","(","analyst"," ","but","tasks","detection","very","streaming","as","main","̈","there","four","effort","batch","related","the","algorithms","ornitz","most","in","unsupervised","-","first","fourth","learning","setting","configure","diversity","to","handle","active","section",",","rieck","context","on","akoglu","nature","many","explainability","g","are","macha","well","shortcomings","work","minimize","anomalies","lacking","anomaly","second","a","third","data","false","principled","do","and","by","2018","2013","way","has","prior","positives","enhancing","designed","discovered","detector",")"],"title":"异常检测 任务 缺点","title_tokens":["任务","检测","缺点","异常"," "]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/AADE/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-%E4%BB%BB%E5%8A%A1/","text":"Anomaly detection (AD) task corresponds to identifying the true anomalies from a given set of data instances. AD algorithms score the data instances and produce a ranked list of candidate anomalies, which are then analyzed by a human to discover the true anomalies.","text_tokens":["task","anomalies","corresponds","anomaly","of","a",".","score","human","(","data","produce"," ","and","detection","by","to","from","ad","analyzed",",","ranked","candidate","discover","true","list","identifying","the","set","are","instances","algorithms","then","given","which",")"],"title":"异常检测 任务","title_tokens":["检测","任务","异常"," "]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/AADE/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/","text":"Explanations are useful for debugging, and a good explanation can inspire confidence in end-users. Traditionally, rule set based descriptions (such as in DNF) have been popular because these are easy for users to understand","text_tokens":["based","explanation",")",".","a","in","(","-","inspire"," ","traditionally","understand","and","descriptions","such","have","explanations","to","easy","can","dnf",",","as","good","users","for","debugging","end","because","these","popular","set","are","confidence","rule","been","useful"],"title":"异常检测 可解释性","title_tokens":["解释性","检测","解释","可","异常"," "]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-LDAPCA/","text":"“同样作为线性降维方法，PCA是有监督的降维算法，而LDA是无监督的降维算法。虽然在原理或应用方面二者有一定的区别，但是从这两种方法的数学本质出发，我们不难发现二者有很多共通的特性。” “首先将LDA扩展到多类高维的情况，以和问题1中PCA的求解对应。假设有N个类别，并需要最终将特征降维至d维。因此，我们要找到一个d维投影超平面 ，使得投影后的样本点满足LDA的目标——最大化类间距离和最小化类内距离。”","text_tokens":["并","超平面","，","目标","求解","扩展","方面","降维","中","最大化","一个","最小化","但是","满足","发现","我们","多类","特征","到","维","无","和","使得","类别","点","是"," ","情况","最小","后","大化","对应","不难","假设","类间","—","原理","出发","”","在","共通","pca","“","找到","n","算法","1","首先","数学","降维至","要","线性","平面","一定","有","方法","而","从","d","两种","。","以","类内","个","需要","很多","这","应用","作为","本质","或","区别","最终","二者","同样","最大","距离","问题","因此","将","投影","虽然","lda","监督","样本","特性","高维","的"],"title":"维度灾难特征提取LDAPCA","title_tokens":["灾难","特征","特征提取","ldapca","维度","提取"]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/","text":"主成分分析 “在机器学习领域中，我们对原始数据进行特征提取，有时会得到比较高维的特征向量。在这些向量所处的高维空间中，包含很多的冗余和噪声。我们希望通过降维的方式来寻找数据内部的特性，从而提升特征表达能力，降低训练复杂度。主成分分析（Principal Components Analysis，PCA）作为降维中最经典的方法，至今已有100多年的历史，它属于一种线性、非监督、全局的降维算法，是面试中经常被问到的问题” “PCA旨在找到数据中的主成分，并利用这些主成分表征原始数据，从而达到降维的目的。” 主成分分析是一个非监督式算法，它用来创造原始特征的线性组合。新创造出来的特征他们之间都是正交的，也就是没有关联性。具体来说，这些新特征是按它们本身变化程度的大小来进行排列的。第一个主成分代表了你的数据集中变化最为剧烈的特征，第二个主成分代表了变化程度排在第二位的特征，以此类推。 因此，你可以通过限制使用主成分的个数来达到数据降维的目的。例如，你可以仅采用能使累积可解释方差为90%的主成分数量。 “在黄线所处的轴上，数据分布得更为分散，这也意味着数据在这个方向上方差更大。在信号处理领域，我们认为信号具有较大方差，噪声具有较小方差，信号与噪声之比称为信噪比。信噪比越大意味着数据的质量越好，反之，信噪比越小意味着数据的质量越差。由此我们不难引出PCA的目标，即最大化投影方差，也就是让数据在主轴上投影的方差最大。” 你需要在使用主成分分析之前，对数据进行归一化处理。否则，原始数据中特征值数量级最大的那个特征将会主导你新创造出来的主成分特征。 优点 ：主成分分析是一项多用途技术，实用效果非常好。它部署起来快速、简单，也就是说，你可以很方便地测试算法性能，无论使用还是不使用主成分分析。此外，主成分分析还有好几种变体和扩展(如：核主成分分析(kernel PCA)，稀疏主成分分析(sparse PCA))，用以解决特定的问题。 缺点 ：新创造出来的主成分并不具备可解释性，因而在某些情况下，新特征与应用实际场景之间很难建立起联系。此外，你仍然需要手动设置、调整累积可解释方差的阈值。 实现 ： Python - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html R - https://stat.ethz.ch/R-manual/R-devel/library/stats/html/prcomp.html 最大方差理论 “至此，我们从最大化投影方差的角度解释了PCA的原理、目标函数和求解方法。其实，PCA还可以用其他思路进行分析，比如从最小回归误差的角度得到新的目标函数。但最终我们会发现其对应的原理和求解方法与本文中的是等价的。另外，由于PCA是一种线性降维方法，虽然经典，但具有一定的局限性。我们可以通过核映射对PCA进行扩展得到核主成分分析（KPCA），也可以通过流形映射的降维方法，比如等距映射、局部线性嵌入、拉普拉斯特征映射等，对一些PCA效果不好的复杂数据集进行非线性降维操作。” 最小平方误差理论 “我们还是考虑二维空间中的样本点，如图4.2所示。上一节求解得到一条直线使得样本点投影到该直线上的方差最大。从求解直线的思路出发，很容易联想到数学中的线性回归问题，其目标也是求解一个线性函数使得对应直线能够更好地拟合样本点集合。如果我们从这个角度定义PCA的目标，那么问题就会转化为一个回归问题。” “顺着这个思路，在高维空间中，我们实际上是要找到一个d维超平面，使得数据点到这个超平面的距离平方和最小。以d=1为例，超平面退化为直线，即把样本点投影到最佳直线，最小化的就是所有点到直线的距离平方之和，如图4.3所示。” “最佳直线ω与最大方差法求解的最佳投影方向一致，即协方差矩阵的最大特征值所对应的特征向量，差别仅是协方差矩阵Σ的一个倍数，以及常数偏差，但这并不影响我们对最大值的优化。 ·总结与扩展· 至此，我们从最小平方误差的角度解释了PCA的原理、目标函数和求解方法。不难发现，这与最大方差角度殊途同归，从不同的目标函数出发，得到了相同的求解方法。”","text_tokens":["相同","引出","超平面","定义","轴","求解","一些","类推","由此","也","decomposition","思路","最小化","原始数据","：","法","原始","创造","4.2","方向","仅","主轴","特征","差别","组合","那个","r","另外","至今已有","大小","复杂","）","还有","表征","平方","较","学习"," ","就是说","也就是说","对应","使用","实际上","100","将会","多年","部署","稀疏","等","使","特定","原理","回归","误差","在","属于","pca","简单","非线性","非常","算法","1","有时","方式","代表","阈值","集","具备","退化","平面","问到","一定","点到","其他","量级","排列","影响","d","来说","以此类推","成分","。","最大值","此类","所处","很多","应用","好几","协方差","4.3","数据","越小","components","最大","html","还是","距离","采用","优化","联系","就","正交","总结","冗余","之间","样本","转化","对","小","的","learn","提取",")","进行","并","归一化","希望","主导","反之","扩展","限性","与","目的","向量","表达能力","嵌入","集中","该","经常","发现","特征向量","我们","认为","之前","黄线","了","以此","解释性","降维中","本文","顺着","最","缺点","不","第一个","是","机器","集合","个数","最小","sklearn","都","prcomp","全局","旨在","快速","stable","设置","实际","不难","（","好几种","第二位","同归","变化","经典","测试","质量","起来","变体","比","能力","调整","多用","具体来说","它们","二维","library","σ","数学","无论","线性","通过","以及","为","第二","排","·","generated","限制","这","局限","kpca","考虑","拉普拉斯","几种","提升","用途","解决","没有","函数","把","越大","从而","平方和","为例","操作","、","manual","高","虽然","它","寻找","噪声","本身","方便","建立","仍然","拉斯","很难","维空间","意味","，","scikit","ethz",".","剧烈","新","表达","降维","二位","中","一个","关联性","sparse","称为","其实","更为","普拉斯","普拉","到","会","分散","维","好","信号","映射","内部","等价","常数","技术","性能","实现","解释","ω","一条","核","比较","kernel","得到","大化","领域","越","某些","https","矩阵","训练","所示","局限性","容易","场景","达到","程度","此外","其","大值",":","数据分布","但","”","“","找到","例如","核主","来","能够","用以","最为","方法","能","意味着","如果","信号处理","效果","如","需要","用","用来","因而","下","较大","优点","手动","不同","你","问题","因此","包含","ch","多用途","特性","拟合","数量","那么","信噪比","起","python","他们","让","目标","方差","/","不好","具体","http","之","principal","分析","关联","最大化","线性组合","第一","这个","modules","二个","可以","stats","越差","殊途同归","局部","和","使得","按","点","这些","实用","复杂度","(","情况","分布","更好","拉普","出来","可","即","还","处理","具有","直线","理论","上","至此","就是","得","式","数量级","出发","非","stat","analysis","如图","被","第二个","-","殊途","地","至今","主","要","已有","利用","比如","偏差","=","90%","所","最佳","从","以","由于","所有","累积","否则","devel","作为","等距","角度","最终","一种","一致","流形","历史","org","特征提取","特征值","倍数","一节","降低","更大","空间","联想","投影","监督","高维","一项","很","面试"],"title":"维度灾难特征提取主成分分析","title_tokens":["成分","灾难","主","分析","特征","特征提取","维度","提取"]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/","text":"线性判别分析 “线性判别分析（Linear Discriminant Analysis，LDA）是一种有监督学习算法，同时经常被用来对数据进行降维。它是Ronald Fisher在1936年发明的，有些资料上也称之为Fisher LDA（Fisher’s Linear Discriminant Analysis）。LDA是目前机器学习、数据挖掘领域中经典且热门的一种算法。” 线性判别分析不是隐含狄利克雷分布，它同样用来构造原始特征集的线性组合。但与主成分分析不同，线性判别分析不会最大化可解释方差，而是最大化类别间的分离程度。 因此，线性判别分析是一种监督式学习方式，它必须使用有标记的数据集。那么，线性判别分析与主成分分析，到底哪种方法更好呢？这要视具体的情况而定，“没有免费的午餐”原理在这里同样适用。 线性判别分析同样依赖于特征值的数量级，你同样需要先对特征值做归一化处理。 优点 ：线性判别分析是一种监督式学习，基于这种方式获取到的特征可以(但并不总是能)提升模型性能。此外，线性判别分析还有一些变体(如二次线性判别分析)，可用来解决特定的问题。 缺点 ：与主成分分析一样，新创造出来的特征不具有可解释性。而且，你同样要手动设置、调整需要保留的特征数量。线性判别分析需要已经标记好的数据，因此，这也让它更加接地气儿。 实现 ： Python - http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis R - https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/lda.html","text_tokens":["先对","标记","python","进行","mass","并","，","热门","归一化","必须","让","scikit",".","ethz","s","新","1936","方差","同时","一些","/","发明","具体","这种","http","赖于","降维","有些","与","不会","分析","也","中","线性组合","最大化","分离","做","：","原始","创造","modules","’","种","而定","经常","discriminant","特征","_","到","组合","可以","接地","好","构造","哪","r","解释性","这要","缺点","）","还有","类别","不","性能","学习","是","呢","解释","(","保留","依赖于"," ","气儿","实现","机器","情况","隐含","分布","更好","狄利克","sklearn","出来","大化","可","领域","处理","资料","使用","具有","stable","https","依赖","适用","上","设置","而是","二次","而且","（","更加","程度","此外","特定","式","原理","数量级","免费","？","stat","analysis",":","但","经典","”","被","在","变体","这里","“","ronald","调整","-","不是","fisher","算法","lineardiscriminantanalysis","library","方式","主","判别","间","要","来","linear","数据挖掘","集","线性","之为","量级","有","方法","能","。","年","目前","成分","到底","获取","generated","挖掘","如","devel","需要","这","用来","一种","雷","优点","org","提升","同样","手动","且","午餐","特征值","没有","解决","称之为","不同","判别分析","数据","你","最大","html","基于","、","问题","因此","manual","可用","ch","它","lda","监督","总是","模型","视","对","已经","数量","#","的","learn","那么","一样",")"],"title":"维度灾难特征提取线性判别分析","title_tokens":["线性","灾难","分析","判别","判别分析","特征","特征提取","维度","提取"]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E8%87%AA%E7%BC%96%E7%A0%81%E6%9C%BA/","text":"自编码机 自编码机是一种人工神经网络，它是用来重新构建原始输入的。例如，图像自编码机是训练来重新表征原始数据的，而非用以区分图片里面的小猫、小狗。 但这有用吗？这里的关键，是在隐含层搭建比输入层和输出层更少数量的神经元。这样，隐含层就会不断学习如何用更少的特征来表征原始图像。 因为是用输入图像来作为目标输出，自编码机被视为无监督学习。它们可被直接使用（如：图像压缩）或按顺序堆叠使用（如：深度学习）。 优点 ：自编码机是人工神经网络中的一种，这表示它们对某些特定类型的数据表现会非常好，比如图像和语音数据。 缺点 ：自编码机是一种人工神经网络。这就是说，它们的优化需要更多的数据来进行训练。它们并不能作为一般意义上的数据降维算法来用。 实现 ： Python - https://keras.io/ R - http://mxnet.io/api/r/index.html","text_tokens":["python","表示","视为","进行","神经元","并","，",".","目标","/","http","降维","mxnet","中","原始数据","：","原始","io","index","编码","特征","图像压缩","会","直接","这就是说","好","r","重新","堆叠","神经网","少","无","和","自","按","）","缺点","表征","网络","学习","是","实现"," ","因为","隐含","就是说","可","某些","使用","类型","人工","训练","更","https","搭建","表现","上","就是","（","特定","？","非","小猫","机是",":","但","不断","被","在","这里","比","-","不能","非常","算法","它们","例如","层","来","用以","意义","这样","有用吗","比如","顺序","神经","关键","而","人工神经网络","深度","。","如","需要","用","这","作为","或","用来","一种","优点","图片","压缩","图像","区分","一般","输入","数据","html","keras","、","优化","就","如何","构建","有用","它","多","监督","小狗","更少","里面","对","数量","机","api","的","语音","输出"],"title":"维度灾难特征提取自编码机","title_tokens":["灾难","机","编码","特征","特征提取","维度","提取","自"]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95/","text":"和集簇方法类似，降维追求并利用数据的内在结构，目的在于使用较少的信息总结或描述数据。 这一算法可用于可视化高维数据或简化接下来可用于监督学习中的数据。许多这样的方法可针对分类和回归的使用进行调整。 例子： 主成分分析（Principal Component Analysis (PCA)） 主成分回归（Principal Component Regression (PCR)） 偏最小二乘回归（Partial Least Squares Regression (PLSR)） Sammon 映射（Sammon Mapping） 多维尺度变换（Multidimensional Scaling (MDS)） 投影寻踪（Projection Pursuit） 线性判别分析（Linear Discriminant Analysis (LDA)） 混合判别分析（Mixture Discriminant Analysis (MDA)） 二次判别分析（Quadratic Discriminant Analysis (QDA)） 灵活判别分析（Flexible Discriminant Analysis (FDA)） 优点： 可处理大规模数据集 无需在数据上进行假设 缺点： 难以搞定非线性数据 难以理解结果的意义","text_tokens":["进行","理解","并","，","针对","内在","降维","principal","目的","中","分析","可视化","：","变换","多维","结构","discriminant","规模","下来","结果","许多","mds","和","映射","缺点","）","flexible","component","寻踪","接下","学习","("," ","least","sammon","难以","无需","最小","projection","大规","可","处理","二乘","使用","偏","上","二次","mapping","（","假设","回归","plsr","描述","analysis","pcr","尺度","partial","在","pca","quadratic","调整","squares","multidimensional","非线性","算法","信息","在于","主","判别","集簇","较少","利用","linear","集","这样","scaling","意义","线性","fda","灵活","大规模","方法","。","成分","类似","或","qda","mda","简化","优点","搞定","mixture","这一","数据","判别分析","regression","混合","例子","投影","用于","总结","lda","分类","接下来","监督","可视","pursuit","高维","追求","的",")"],"title":"维度灾难 特征提取 降维算法","title_tokens":["算法","降维","灾难","特征","特征提取","维度","提取"," "]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/","text":"特征提取 特征提取是用来创造一个新的、较小的特征集，但仍能保留绝大部分有用的信息。值得再提的是，特征选取是用来保留原始特征集中的一部分子特征集，而特征提取则是创造全新的特征集。 跟特征选取一样，某些算法内部已经具有了特征提取的机制。最好的案例就是深度学习，它可以通过每一层隐神经层，提取出越来越有用的能表征原始数据的特征。我们在“深度学习”部分已给出相关的讲解。 作为独立的任务，特征提取可以是非监督式的(如主成分分析)或监督式的(如线性判别分析)。","text_tokens":["则","，","新","分析","一个","原始数据","原始","创造","集中","全新","我们","特征","可以","了","内部","表征","一层","较","学习","是","(","保留"," ","讲解","某些","提","具有","选取","绝大","就是","任务","式","值得","仍","但","一部分","”","机制","在","“","算法","部分","信息","层","判别","大部","集","线性","通过","案例","神经","隐","一部","而","能","深度","。","成分","大部分","如","越来","作为","或","用来","跟","子","出","再","特征提取","如主","数据","相关","判别分析","已","、","独立","最好","越来越","有用","每","它","绝大部分","监督","已经","小","的","是非","一样","提取","给出",")"],"title":"维度灾难特征提取","title_tokens":["灾难","特征","特征提取","维度","提取"]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE-%E7%89%B9%E5%BE%81%E9%80%89%E5%8F%96-%E6%96%B9%E5%B7%AE%E9%98%88%E5%80%BC/","text":"方差阈值 方差阈值会摒弃掉观测样本那些观测值改变较小的特征(即，它们的方差小于某个设定的阈值)。这样的特征的价值极小。 举例来说，如果你有一份公共健康数据，其中96%的人都是35岁的男性，那么去掉“年龄”和“性别”的特征也不会损失重要信息。 由于方差阈值依赖于特征值的数量级，你应该对特征值先做归一化处理。 优点 ：使用方差阈值方式进行数据降维只需一个非常可靠的直觉：特征值不怎么改变的特征，不会带来什么有用的信息。这是在你建模初期进行数据降维相对安全的一种方式。 缺点 ：如果你正在解决的问题并不需要进行数据降维，即便使用了方差阈值也几乎没有什么作用。此外，你需要手工设置、调整方差阈值，这个过程相当具有技术含量。我们建议从一个保守(也就是，较低)的阈值开始。 实现 ： Python - http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html R - https://www.rdocumentation.org/packages/caret/versions/6.0-76/topics/nearZeroVar","text_tokens":["packages","python","不怎么","重要","进行","，","scikit","归一化",".","方差","/","http","健康","赖于","降维","不会","摒弃","也","做","一个","公共","其中","：","这个","怎么","modules","我们","特征","96%","_","会","feature","某个","了","年龄","观测","r","什么","和","值","缺点","价值","正在","技术","举例来说","相对","较","开始","是","(","相当","实现","依赖于"," ","sklearn","极小","都","35","几乎","即","小于","处理","使用","具有","selection","stable","https","建模","安全","依赖","手工","设置","含量","topics","就是","variancethreshold","此外","只","数量级","举例","去掉",":","”","低","初期","versions","在","掉","“","需","调整","-","非常","www","它们","信息","方式","caret","可靠","保守","阈值","改变","这样","量级","有","从","岁","人","来说","一份","。","由于","如果","即便","rdocumentation","generated","那些","需要","这","一种","优点","应该","作用","过程","org","设定","建议","性别","特征值","解决","没有","技术含量","数据","你","html","问题","、","直觉","6.0","男性","带来","有用","损失","样本","并不需要","76","对","nearzerovar","小","数量","先","的","learn","那么",")"],"title":"维度灾难特征选取方差阈值","title_tokens":["选取","灾难","特征","阈值","维度","方差"]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE-%E7%89%B9%E5%BE%81%E9%80%89%E5%8F%96-%E7%9B%B8%E5%85%B3%E6%80%A7%E9%98%88%E5%80%BC/","text":"相关性阈值 相关性阈值会去掉那些高度相关的特征(亦即，这些特征的特征值变化与其他特征非常相似)。它们提供的是冗余信息。 举例来说，如果你有一个房地产数据，其中两个特征分别是“房屋面积（单位：平方英尺）”和“房屋面积（单位：平方米）”，那么，你就可以去掉其中的任何一个（这非常安全，也不会给你的模型带来任何负面影响）。 问题是，你该去掉哪一个特征呢？首先，你应该计算所有特征对的相关系数。而后，如果某个特征对的相关系数大于设定的阈值，那你就可以去掉其中平均绝对相关系数较大的那一个。 优点 ：使用相关性阈值同样只需一个可靠的直觉：相似的特征提供了冗余的信息。对于某些含有强相关性特征较多的数据集，有些算法的稳健性并不好，因此，去掉它们可以提升整个模型的性能(计算速度、模型准确度、模型稳健性，等等)。 缺点 ：同样，你还是必须手动去设置、调整相关性阈值，这同样是个棘手且复杂的过程。此外，如果你设置的阈值过低，那么你将会丢失掉一些有用的信息。无论在什么时候，我们都更倾向于使用那些内置了特征选取的算法。对于没有内置特征提取的算法，主成分分析是一个很好的备用方案。 实现 ： Python - https://gist.github.com/Swarchal/881976176aaeb21e8e8df486903e99d6 R - https://www.rdocumentation.org/packages/caret/versions/6.0-73/topics/findCorrelation","text_tokens":["packages","python","整个","面积","并","，","必须",".","过低","时候","给","不好","一些","/","与","有些","不会","也","亦","分析","一个","其中","计算速度","：","任何","于","该","我们","特征","棘手","会","swarchal","可以","房屋","某个","了","好","英尺","哪","r","什么","关系","分别","和","复杂","大于","缺点","）","com","等等","性能","平方","举例来说","这些","对于","较","github","是","稳健","(","呢","实现"," ","计算","相关性","881976176aaeb21e8e8df486903e99d6","都","即","某些","使用","https","更","选取","绝对","安全","设置","topics","（","此外","高度","只","？","变化","举例","去掉","两个",":","去","”","versions","地产","在","平方米","需","“","调整","掉","-","非常","很","算法","稳健性","它们","负面","信息","首先","www","主","caret","可靠","无论","阈值","集","单位","含有","平均","其他","备用","有","倾向","影响","来说","负面影响","。","所有","成分","如果","rdocumentation","个","那些","那","而后","丢失","这","相关系数","73","速度","较大","优点","应该","过程","平方英尺","方案","特征提取","org","设定","房地","同样","提升","手动","且","特征值","没有","gist","房地产","数据","相关","你","还是","问题","、","强","直觉","因此","6.0","就","带来","将","有用","冗余","多","相似","模型","系数","提供","对","准确度","的","内置","准确","那么","提取","findcorrelation",")"],"title":"维度灾难特征选取相关性阈值","title_tokens":["相关性","选取","灾难","相关","特征","阈值","维度"]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE-%E7%89%B9%E5%BE%81%E9%80%89%E5%8F%96-%E9%80%90%E6%AD%A5%E6%90%9C%E7%B4%A2/","text":"逐步搜索 逐步搜索是一个基于序列式搜索的监督式特征选取算法。它有两种形式：前向搜索和反向搜索。 对于前向逐步搜索，你从没有任何特征开始。接着，从候选特征集中，选择一个特征来训练模型；然后，保存模型性能最好对应的那个特征；再往下，你不断往训练模型的特征集中添加特征，一次添加一个特征，直到你模型的性能不再提升。 反向逐步搜索的过程相同，只不过顺序相反：从把所有的特征都用于训练模型，接着一次性移除一个特征，直到模型的性能骤降。 我们提及这一算法纯粹是源于某些历史原因。尽管很多教科书都把逐步搜索算法作为一个有效的方法，但它所表现出来的性能总是不及其它监督式方法，比如正则化。逐步搜索有很多明显的缺陷，最致命的一点就是它是一个贪心算法，无法面对未来变化的冲击。我们并不推荐这个算法。","text_tokens":["选择","相同","并","，","保存","面对","一个","：","任何","这个","搜索","集中","冲击","我们","特征","那个","一点","形式","和","未来","最","移除","源于","化","性能","不","尽管","对于","开始","推荐","是"," ","贪心","搜索算法","向搜索","都","出来","对应","接着","某些","训练","选取","表现","反向","前向","教科","就是","然后","不再","序列","式","变化","但","候选","添加","不断","无法","算法","骤降","不及","来","一次","比如","顺序","教科书","有效","再往","所","原因","有","方法","从","两种","纯粹","。","往","所有","只不过","逐步","前","很多","作为","下","一次性","直到","过程","历史","致命","明显","提升","；","没有","正则","这一","把","你","基于","最好","用于","它","监督","相反","总是","模型","其它","缺陷","提及","的","不过"],"title":"维度灾难特征选取逐步搜索","title_tokens":["搜索","逐步","选取","灾难","特征","维度"]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE-%E7%89%B9%E5%BE%81%E9%80%89%E5%8F%96-%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/","text":"遗传算法是可用于不同任务的一大类算法的统称。它们受进化生物学与自然选择的启发，结合变异与交叉，在解空间内进行高效的遍历搜索。这里有一篇非常棒的简介：“遗传算法背后的原理引入”。 在机器学习领域，遗传算法主要有两大用处。 其一，用于最优化，比如去找神经网络的最佳权重。 其二，是用于监督式特征提取。 这一用例中，“基因”表示单个特征，同时“有机体”表示候选特征集。“种群体”内的每一个有机体都会基于其适应性进行评分，正如在测试数据集上进行模型性能测试。最能适应环境的有机体将会生存下来，并不断繁衍，一直迭代，直至最终收敛于某个最优的解决方案。 优点 ：在穷举搜索不可行的情况下，对高维数据集使用遗传算法会相当有效。当你的算法需要预处理数据却没有内置的特征选取机制(如最近邻分类算法），而你又必须保留最原始的特征(也就是不能用任何主成分分析算法)，遗传算法就成了你最好的选择。这一情况在要求透明、可解释方案的商业环境下时有发生。 缺点 ：遗传算法为你解决方案的实施带来了更高的复杂度，而多数情况下它们都是不必要的麻烦。如果可能的话，主成分分析或其它内置特征选取的算法将会更加高效和简洁。 实现 ： Python - https://pypi.python.org/pypi/deap R - https://cran.r-project.org/web/packages/GA/vignettes/GA.html","text_tokens":["最优化","packages","python","选择","表示","遗传","进行","遗传算法","棒","并","受","，","必须","神经网络","测试数据","当",".","同时","/","可行","例中","与","cran","也","分析","麻烦","一个","deap","：","集上","任何","要求","原始","于","搜索","种","机体","大用","特征","下来","这一用","可能","会","最能","pypi","某个","了","其二","解决方案","生存","r","发生","神经网","和","复杂","最","缺点","进化","）","遍历","性能","不","权重","解","网络","直至","project","学习","是","复杂度","相当","却","(","保留","解释"," ","实现","穷举","单个","机器","情况","自然选择","适应性","正如","收敛","都","可","高效","领域","处理","使用","预处理","更","简洁","https","选取","迭代","生物","一篇","引入","用处","将会","就是","web","更加","不必要","最优","任务","式","原理","又","大用处","其","一直","评分",":","去","”","候选","测试","不断","机制","启发","在","邻","这里","“","两","-","不能","商业","群体","非常","算法","它们","主","找","集","背后","统称","比如","有效","的话","神经","适应","最佳","实施","有","而","内","。","成分","时有发生","为","如果","结合","如","需要","用","或","下","最终","自然","优点","多数","方案","org","特征提取","有机","基因","解决","没有","生物学","变异","这一","vignettes","不同","数据","简介","类","你","成","html","基于","适应环境","、","不必","一大","其一","优化","最好","主要","环境","交叉","空间","繁衍","就","用于","带来","透明","高","每","分类","有机体","监督","必要","模型","ga","最近","对","其它","高维","的","内置","提取",")"],"title":"维度灾难特征选取遗传算法","title_tokens":["算法","遗传","遗传算法","选取","灾难","特征","维度"]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE-%E7%89%B9%E5%BE%81%E9%80%89%E5%8F%96/","text":"特征选取 特征选取是从你的数据集中过滤掉不相关或冗余的特征。特征选取与特征提取的关键区别在于：特征选取是从原特征集中选取一个子特征集，而特称提取则是在原特征集的基础上重新构造出一些(一个或多个)全新的特征。 需要注意的是，某些监督式机器学习算法已经具备了内在的特征选取机制：比如正则回归与随机森林。通常，我们是建议一开始优先尝试这些算法，如果它们能匹配上你的问题的话。对此我们已经做过介绍。 作为独立的任务，特征选取既可以是非监督式的(如方差阈值)，又可以是监督式的(比遗传算法)。有必要的话，你还可以把多种方法以某种合理的方式整合在一起。","text_tokens":["遗传","则","遗传算法","，","内在","方差","一些","与","一个","整合","提取","：","集中","匹配","全新","我们","特征","可以","了","构造","重新","做过","介绍","不","这些","开始","学习","是","(","尝试"," ","机器","还","合理","某些","基础","优先","选取","上","既","任务","式","又","回归","随机","机制","在","掉","比","算法","它们","在于","方式","多种","集","阈值","比如","具备","森林","多个","的话","有","关键","方法","而","能","。","以","如果","是从","原","如","需要","某种","注意","作为","或","区别","子","出","一","一起","特征提取","对此","建议","正则","特称","把","数据","你","相关","问题","通常","独立","冗余","监督","必要","已经","的","是非","过滤",")"],"title":"维度灾难特征选取","title_tokens":["特征","维度","选取","灾难"]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE/","text":"在机器学习领域，“维度(Dimensionality)”通常指数据集中的特征数量（即输入变量的个数）。 当特征的个数特别大的时候（相对于数据集中观测样本的数量来说），训练出一个有效的模型，对算法要求就会特别高(即，用现有的算法训练出一个有效的模型特别困难)。这就是所谓的“维度灾难(Curse of Dimensionality)”，特别是对依赖于距离计算的聚类算法而言。 对于“维度灾难”，有位 Quora 用户给出了一个非常好的类比： 假设有一条100码的直线，而你在该直线的某处掉了一枚硬币。要找回硬币并不难，你只需沿着这条线走就行了，最多花上你2分钟时间。 ​ 然后，假设有一个长和宽都是100码的广场，而你是把硬币掉在广场的某个位置。现在再想找回它，可就不容易了，这好比在两个并排的足球场中找针，可能会耗上你几天时间。 ​ 再然后，假设是一个长、宽、高都是100码的立方体，那就好比是在30层楼高的大型体育场内找zhen找针…… ​ 随着维度的增加，在空间中搜索的难度也会变得愈加困难。 Quora链接： https://www.quora.com/What-is-the-curse-of-dimensionality/answer/Kevin-Lacker 这就需要数据降维的办法： 特征选取和特征提取。 “机器学习中的数据维数与现实世界的空间维度本同末离。在机器学习中，数据通常需要被表示成向量形式以输入模型进行训练。但众所周知，对向维向量进行处理和分析时，会极大地消耗系统资源，甚至产生维度灾难。因此，进行降维，即用一个低维度的向量表示原始高维度的特征就显得尤为重要。常见的降维方法有主成分分析、线性判别分析、等距映射、局部线性嵌入、拉普拉斯特征映射、局部保留投影等。”","text_tokens":["随着","表示","重要","进行","系统资源","并","，","一枚",".","当","时候","/","甚至","几天","消耗","赖于","降维","与","周知","并排","也","中","向量","一个","分析","广场","：","最多","要求","curse","原始","嵌入","于","quora","集中","该","搜索","尤为重要","想","层楼","尤为","宽","现有","特征","普拉斯","普拉","可能","会","本同末离","足球","现实","某个","了","观测","好","形式","分钟","局部","is","what","和","好比","映射","）","产生","com","不","相对","answer","of","对于","学习","是","现在","(","dimensionality","找回","依赖于"," ","计算","一条","众所周知","保留","机器","个数","有位","常见","都","拉普","可","领域","即","办法","处理","资源","https","训练","直线","位置","选取","100","立方体","依赖","容易","上","类比","这条","不难","就是","（","假设","链接","等","就行了","然后","走","只","the","变量","世界","大","两个","沿着","球场",":","时","但","”","低","2","极大","被","在","掉","“","需","显得","-","即用","系统","非常","增加","算法","变得","www","地","时间","判别","特别","要","找","体育","线性","有效","难度","灾难","维数","大型","有","方法","而","内","来说","。","以","成分","体育场","聚类","…","愈加","那","需要","这","用","等距","硬币","出","所谓","再","拉普拉斯","特征提取","而言","会耗","维度","码","找针","指","把","困难","数据","输入","你","成","判别分析","某处","有主","距离","、","通常","​","因此","就","高","这条线","zhen","空间","投影","kevin","它","足球场","样本","用户","模型","对","数量","30","立方","长","对向维","的","lacker","为重","最多花","提取","拉斯","给出",")"],"title":"维度灾难","title_tokens":["维度","灾难"]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E8%81%9A%E7%B1%BB/%E8%81%9A%E7%B1%BB-DBSCAN/","text":"DBSCAN DBSCAN 是一种基于密度的聚类算法，它将样本点的密集区域组成集群；其最新进展是HDBSCAN，它允许集群的密度可变。 优点 ：DBSCAN 不需要假定类球形集群，其性能可以扩展。此外，它不需要每个点都被分配到集群中，这就降低了集群的噪音。 缺点 ：用户必须要调整“epsilon”和“min_sample”这两个超参数来定义集群密度。DBSCAN 对此非常敏感。 实现 ： Python - http://scikit-learn.org/stable/modules/clustering.html#dbscan R - https://cran.r-project.org/web/packages/dbscan/index.html w:【无监督学习】DBSCAN聚类算法原理介绍，以及代码实现","text_tokens":["区域","packages","python","，","scikit","必须",".","定义","min","组成","/","扩展","http","cran","中","：","新进展","modules","index","_","到","可以","了","球形","r","分配","无","和","w","缺点","hdbscan","介绍","超","性能","不","点","project","dbscan","学习","是","实现"," ","每个","都","允许","https","【","stable","集群","web","此外","原理","其","最新进展","两个",":","”","被","“","调整","假定","-","epsilon","非常","算法","】","要","来","可变","以及","密集","进展","最新","。","聚类","需要","这","一种","优点","密度","org","对此","；","sample","类","html","基于","clustering","降低","就","将","它","监督","样本","用户","#","敏感","的","learn","参数","噪音","代码"],"title":"聚类DBSCAN","title_tokens":["dbscan","聚类"]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E8%81%9A%E7%B1%BB/%E8%81%9A%E7%B1%BB-DBSCAN/#wdbscan","text":"","text_tokens":[],"title":"w:【无监督学习】DBSCAN聚类算法原理介绍，以及代码实现","title_tokens":["以及","算法","】","【",":","聚类","介绍","，","无","原理","w","学习","监督","dbscan","实现","代码"]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E8%81%9A%E7%B1%BB/%E8%81%9A%E7%B1%BB-EM%E7%AE%97%E6%B3%95/","text":"“K均值算法的收敛性。” EM算法是基于模型的聚类方法，是在概率模型中寻找参数最大似然估计的算法，其中概率模型依赖于无法观测的隐藏变量。E步估计隐含变量，M步估计其他参数，交替将极值推向最大。 EM算法比K-means算法计算复杂，收敛也较慢，不适于大规模数据集和高维数据，但比K-means算法计算结果稳定、准确。EM经常用在机器学习和计算机视觉的数据集聚（Data Clustering）领域。 特点： E:给定参数与观测数据下对未观测数据的条件概率分布的期望 M：求使条件概率分布期望最大下的参数值 优点： 比K-means稳定、准确 缺点： 计算复杂且收敛慢，依赖于初始参数假设","text_tokens":["计算结果","，","交替","似然","赖于","与","也","中","其中","：","经常","规模","数值","结果","均值","观测","概率分布","视觉","收敛性","和","em","条件","复杂","缺点","）","不","学习","是","依赖于"," ","计算","机器","隐含","分布","收敛","稳定","大规","期望","领域","较慢","依赖","概率","推向","（","假设","给定","适于","变量",":","初始","但","”","无法","在","“","比","-","算法","e","计算机","集","大规模","其他","集聚","概率模型","方法","k","。","聚类","慢","用","下","特点","优点","步","隐藏","data","估计","且","数据","最大","基于","、","clustering","means","参数值","将","寻找","模型","m","对","高维","算机","求使","的","准确","未","参数","极值"],"title":"聚类EM算法","title_tokens":["算法","em","聚类"]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E8%81%9A%E7%B1%BB/%E8%81%9A%E7%B1%BB-GMM-EM/","text":"“高斯混合模型（Gaussian Mixed Model，GMM）也是一种常见的聚类算法，与K均值算法类似，同样使用了EM算法进行迭代计算。高斯混合模型假设每个簇的数据都是符合高斯分布（又叫正态分布）的，当前数据呈现的分布就是各个簇的高斯分布叠加在一起的结果。 图5.6是一个数据分布的样例，如果只用一个高斯分布来拟合图中的数据，图中所示的椭圆即为高斯分布的二倍标准差所对应的椭圆。直观来说，图中的数据明显分为两簇，因此只用一个高斯分布来拟和是不太合理的，需要推广到用多个高斯分布的叠加来对数据进行拟合。图5.7是用两个高斯分布的叠加来拟合得到的结果。这就引出了高斯混合模型，即用多个高斯分布函数的线形组合来对数据分布进行拟合。理论上，高斯混合模型可以拟合出任意类型的分布。” “高斯混合模型的核心思想是什么？它是如何迭代计算的？” “说起高斯分布，大家都不陌生，通常身高、分数等都大致符合高斯分布。因此，当我们研究各类数据时，假设同一类的数据符合高斯分布，也是很简单自然的假设；当数据事实上有多个类，或者我们希望将数据划分为一些簇时，可以假设不同簇中的样本各自服从不同的高斯分布，由此得到的聚类算法称为高斯混合模型。 高斯混合模型的核心思想是，假设数据可以看作从多个高斯分布中生成出来的。在该假设下，每个单独的分模型都是标准高斯模型，其均值$μi$和方差$Σi$是待估计的参数。此外，每个分模型都还有一个参数$πi$，可以理解为权重或生成数据的概率。高斯混合模型的公式为” “高斯混合模型是一个生成式模型。可以这样理解数据的生成过程，假设一个最简单的情况，即只有两个一维标准高斯分布的分模型N(0,1)和N(5,1)，其权重分别为0.7和0.3。那么，在生成第一个数据点时，先按照权重的比例，随机选择一个分布，比如选择第一个高斯分布，接着从N(0,1)中生成一个点，如−0.5，便是第一个数据点。在生成第二个数据点时，随机选择到第二个高斯分布N(5,1)，生成了第二个点4.7。如此循环执行，便生成出了所有的数据点。 然而，通常我们并不能直接得到高斯混合模型的参数，而是观察到了一系列数据点，给出一个类别的数量K后，希望求得最佳的K个高斯分模型。因此，高斯混合模型的计算，便成了最佳的均值μ，方差Σ、权重π的寻找，这类问题通常通过最大似然估计来求解。遗憾的是，此问题中直接“直接使用最大似然估计，得到的是一个复杂的非凸函数，目标函数是和的对数，难以展开和对其求偏导。” “在这种情况下，可以用上一节已经介绍过的EM算法框架来求解该优化问题。EM算法是在最大化目标函数时，先固定一个变量使整体函数变为凸优化函数，求导得到最值，然后利用最优参数更新被固定的变量，进入下一个循环。具体到高斯混合模型的求解，EM算法的迭代过程如下。 ” “首先，初始随机选择各参数的值。然后，重复下述两步，直到收敛。 （1）E步骤。根据当前的参数，计算每个点由某个分模型生成的概率。 （2）M步骤。使用E步骤估计出的概率，来改进每个分模型的均值，方差和权重。 也就是说，我们并不知道最佳的K个高斯分布的各自3个参数，也不知道每个数据点究竟是哪个高斯分布生成的。所以每次循环时，先固定当前的高斯分布不变，获得每个数据点由各个高斯分布生成的概率。然后固定该生成概率不变，根据数据点和生成概率，获得一个组更佳的高斯分布。循环往复，直到参数的不再变化，或者变化非常小时，便得到了比较合理的一组高斯分布。","text_tokens":["引出","当","展开","求解","一些","这种","由此","也","符合","μ","变为","分","组合","4.7","某个","两步","em","复杂","）","还有"," ","每个","重复","就是说","也就是说","样例","后","其求","对应","观察","求导","使用","下述","而是","等","便","使","π","陌生","时","随机","往复","太","分为","i","在","简单","非常","算法","1","求得","e","这样",",","生成","分数","多个","偏导","更新","有","来说","。","个","任意","过程","来拟","估计","按照","数据","进入","最大","说起","优化","就","如何","将","0.3","叠加","样本","模型","当前","对","已经","只有","的","参数",")","进行","并","希望","同","比例","与","该","我们","均值","直接","点时","直观","了","分别","最","各自","不","第一个","是","都","对数","类型","每次","各个","概率","（","假设","步骤","研究","变量","究竟","变化","身高","两个","2","只用","便成","不能","n","σ","一组","通过","遗憾","生成式","为","第二","公式","这","一维","自然","知道","函数","改进","类","、","通常","一个点","循环往复","它","如此","一类","寻找","各类","正态分布","理解","簇中","，","大家","中","一个","簇时","称为","结果","到","一系","获得","值","所以","介绍","如下","循环","5","二倍","收敛","比较","得到","大化","图","高斯","所示","迭代","gaussian","小时","整体","此外","推广","叫","？","其","数据分布","”","“","0","来","看作","标准","高斯分布","聚类","如果","一系列","系列","如","需要","两簇","用","或","下","直到","一起","明显","不同","并不知道","混合","问题","不变","因此","凸","心思","一个组","框架","gmm","拟合","m","数量","事实","先","那么","给出","选择","核心思想","目标","方差","似然","待","具体","各","事实上","第一","最大化","0.7","二个","可以","此","model","单独","图中","−","什么","和","标准差","执行","类别","权重","点","(","计算","情况","分布","难以","常见","划分","出来","即","接着","合理","然而","理论","上","就是","最优","然后","不再","又","非","不知","初始","过","$","呈现","或者","3","被","第二个","即用","思想","首先","mixed","根据","利用","凸函数","比如","便是","所","最佳","k","从","所有","哪个","5.6","类似","椭圆","最值","线形","出","一种","更佳","同样","；","0.5","簇","一节","由","固定","服从","核心","5.7","很","大致"],"title":"聚类 GMM EM","title_tokens":["gmm","em","聚类"," "]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E8%81%9A%E7%B1%BB/%E8%81%9A%E7%B1%BB-GMM-Kmeans/","text":"高斯混合模型与K均值算法的相同点是，它们都是可用于聚类的算法；都需要指定K值；都是使用EM算法来求解；都往往只能收敛于局部最优“。而它相比于K均值算法的优点是，可以给出一个样本属于某类的概率是多少；不仅仅可以用于聚类，还可以用于概率密度的估计；并且可以用于生成新的样本点。”","text_tokens":["相同","”","优点","，","仅仅","密度","点","属于","是","求解","“","新","；","估计","不仅","算法","它们","并且","与","相比","指定","混合","收敛","来","一个","都","往往","可","还","于","同点","只能","高斯","生成","使用","用于","它","均值","某类","样本","k","概率","而","可以","多少","模型","。","聚类","局部","最优","不仅仅","的","概率密度","值","em","需要","给出","相同点"],"title":"聚类GMMKmeans","title_tokens":["聚类","gmmkmeans"]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E8%81%9A%E7%B1%BB/%E8%81%9A%E7%B1%BB-Isodata/","text":"“当K值的大小不确定时，可以使用ISODATA算法。ISODATA的全称是迭代自组织数据分析法。在K均值算法中，聚类个数K的值需要预先人为地确定，并且在整个算法过程中无法更改。而当遇到高维度、海量的数据集时，人们往往很难准确地估计出K的大小。ISODATA算法就是针对这个问题进行了改进，它的思想也很直观。当属于某个类别的样本数过少时，把该类别去除；当属于某个类别的样本数过多、分散程度较大时，把该类别分为两个子类别。ISODATA算法在K均值算法的基础之上增加了两个操作，一是分裂操作，对应着增加聚类中心数；二是合并操作，对应着减少聚类中心数。ISODATA算法是一个比较常见的算法，其缺点是需要指定的参数比较多，不仅仅需要一个参考的聚类数量Ko，还需要制定3个阈值。下面介绍ISODATA算法的各个输入参数。” “（1）预期的聚类中心数目$Ko$。在ISODATA运行过程中聚类中心数可以变化，$Ko$是一个用户指定的参考值，该算法的聚类中心数目变动范围也由其决定。具体地，最终输出的聚类中心数目常见范围是从Ko的一半，到两倍Ko。 （2）每个类所要求的最少样本数目Nmin。如果分裂后会导致某个子类别所包含样本数目小于该阈值，就不会对该类别进行分裂操作。 （3）最大方差Sigma。用于控制某个类别中样本的分散程度。当样本的分散程度超过这个阈值时，且分裂后满足（1），进行分裂操作。” “（4）两个聚类中心之间所允许最小距离Dmin。如果两个类靠得非常近（即这两个类别对应聚类中心之间的距离非常小），小于该阈值时，则对这两个类进行合并操作。 如果希望样本不划分到单一的类中，可以使用模糊C均值或者高斯混合模型，高斯混合模型会在下一节中详细讲述。”","text_tokens":["数据分析","整个","进行","则","海量","，","针对","近","希望","当","方差","具体","不会","分裂","分析","中","也","一个","指定","运行","详细","法","这个","往往","二是","要求","该","满足","ko","均值","到","会","分散","可以","人为","直观","了","某个","最少","不仅仅","人们","值","预先","大小","自","缺点","）","一半","sigma","合并","介绍","类别","不","决定","集时","c","是","一是"," ","两倍","每个","个数","常见","最小","比较","划分","导致","超过","后","对应","还","数目","小于","即","高斯","使用","基础","允许","迭代","遇到","预期","各个","数","dmin","就是","（","全称","样本数","过多","得","制定","程度","组织","变化","4","其","模糊","两个","时","过","$","分为","”","仅仅","2","或者","无法","更改","3","在","属于","“","思想","增加","非常","算法","1","并且","地","本数","去除","阈值","变动","少时","所","k","着","而","之上","。","聚类","控制","如果","是从","个","isodata","需要","这","子","参考","出","中聚类","单一","最终","下","较大","过程","确定","范围","维度","估计","；","且","不仅","改进","把","数据","输入","类","混合","最大","操作","一节","距离","、","问题","由","包含","高","就","中心","参考值","用于","之间","它","多","nmin","样本","用户","模型","对","数量","小","下面","的","准确","输出","参数","减少","类靠","很","讲述","很难"],"title":"聚类Isodata","title_tokens":["isodata","聚类"]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E8%81%9A%E7%B1%BB/%E8%81%9A%E7%B1%BB-SOM-kmeans/","text":"“自组织映射神经网络与K均值算法的区别如下。 （1）K均值算法需要事先定下类的个数，也就是K的值。而自组织映射神经网络则不用，隐藏层中的某些节点可以没有任何输入数据属于它，因此聚类结果的实际簇数可能会小于神经元的个数。而K均值算法受K值设定的影响要更大一些。 （2）K均值算法为每个输入数据找到一个最相似的类后，只更新这个类的参数；自组织映射神经网络则会更新临近的节点。所以，K均值算法受noise data的影响比较大，而自组织映射神经网络的准确性可能会比K均值算法低（因为也更新了临近节点）。 （3）相比较而言，自组织映射神经网络的可视化比较好，而且具有优雅的拓扑关系图。”","text_tokens":["则","神经元","受","，","神经网络","一些","层中","与","也","一个","可视化","任何","这个","相","结果","均值","可能","会","可以","了","好","神经网","准确性","关系","值","映射","最","自","）","所以","网络","如下"," ","每个","个数","因为","比较","后","小于","图","定下","某些","具有","更","实际","而且","就是","（","比较而言","只","组织","大","”","低","2","3","属于","“","比","找到","算法","1","要","事先","簇数","神经","更新","k","而","影响","。","为","节点","聚类","需要","区别","优雅","拓扑","noise","而言","隐藏","设定","data","不用","；","没有","输入","数据","类","因此","它","相似","临近","可视","的","准确","参数"],"title":"聚类SOMkmeans","title_tokens":["somkmeans","聚类"]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E8%81%9A%E7%B1%BB/%E8%81%9A%E7%B1%BB-SOM/","text":"“自组织映射神经网络（Self-Organizing Map，SOM）是无监督学习方法中一类重要方法，可以用作聚类、高维可视化、数据压缩、特征提取等多种用途。在深度神经网络大为流行的今天，谈及自组织映射神经网络依然是一件非常有意义的事情，这主要是由于自组织映射神经网络融入了大量人脑神经元的信号处理机制，有着独特的结构特点。该模型由芬兰赫尔辛基大学教授Teuvo Kohonen于1981年提出，因此也被称为Kohonen网络。” “自组织映射神经网络本质上是一个两层的神经网络，包含输入层和输出层” “输入层模拟感知外界输入信息的视网膜，输出层模拟做出响应的大脑皮层。输出层中神经元的个数通常是聚类的个数，代表每一个需要聚成的类。训练时采用“竞争学习”的方式，每个输入的样例在输出层中找到一个和它最匹配的节点，称为激活节点，也叫winning neuron；紧接着用随机梯度下降法更新激活节点的参数；同时，和激活节点临近的点也根据它们距离激活节点的远近而适当地更新参数。这种竞争可以通过神经元之间的横向抑制连接（负反馈路径）来实现。自组织映射神经网络的输出层节点是有拓扑关系的。这个拓扑关系依据需求确定，如果想要一维的模型，那么隐藏节点可以是“一维线阵”；如果想要二维的拓扑关系，那么就行成一个“二维平面阵”，如图5.8所示。也有更高维度的拓扑关系的，比如“三维栅格阵”，但并不常见。” “怎样设计自组织映射神经网络并设定网络训练参数？ ”","text_tokens":["响应","重要","神经元","并","，","聚成","反馈","神经网络","同时","organizing","层中","这种","梯度","设计","winning","也","中","想要","一个","提取","可视化","法","连接","这个","皮层","于","匹配","该","结构","称为","横向","线阵","特征","人脑","有着","可以","了","下降","信号","神经网","无","关系","和","三维","映射","大学","最","一件","自","）","不","点","网络","学习","是","实现"," ","每个","self","个数","常见","融入","负反馈","需求","做出","样例","赫尔","接着","处理","更","训练","所示","kohonen","上","平面阵","阵","视网","（","提出","等","竞争","5.8","两层","数据压缩","组织","叫","？","视网膜","时","随机","如图","但","”","机制","依据","被","在","“","-","找到","非常","芬兰","它们","路径","信息","方式","远近","用作","层","地","根据","二维","网膜","来","赫尔辛基大学","多种","代表","依然","意义","紧接","比如","通过","平面","流行","栅格","神经","独特","更新","有","方法","而","大脑","map","深度","。","由于","年","节点","聚类","如果","信号处理","事情","抑制","需要","这","用","本质","一维","谈及","教授","som","1981","neuron","特点","拓扑","辛基","大脑皮层","特征提取","确定","大为","隐藏","维度","设定","；","用途","压缩","激活","适当","数据","今天","输入","类","距离","、","由","通常","因此","采用","包含","外界","就","感知","行成","模拟","高","之间","每","它","怎样","teuvo","监督","一类","临近","可视","模型","高维","的","紧接着","赫尔辛","输出","参数","那么","主要","大量"],"title":"聚类SOM","title_tokens":["som","聚类"]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E8%81%9A%E7%B1%BB/%E8%81%9A%E7%B1%BB-kmeans%2B%2B/","text":"“K均值算法的主要缺点如下。 （1）需要人工预先确定初始K值，且该值和真实的数据分布未必吻合。 （2）K均值只能收敛到局部最优，效果受到初始值很大。 （3）易受到噪点的影响。 （4）样本点只能被划分到单一的类中。 ■ K-means++算法 因为 k-means++ 要解决k-means受初始化值影响大这个问题，因此 k-means++ 算法中，最关键的一步就是 初始 点的选取应该足够的离散 K均值的改进算法中，对初始值选择的改进是很重要的一部分。而这类算法中，最具影响力的当属K-means++算法。原始K均值算法最开始随机选取数据集中K个点作为聚类中心，而K-means++按照如下的思想选取K个聚类中心。假设已经选取了n个初始聚类中心（0<n<K），则在选取第n+1个聚类中心时，距离当前n个聚“类中心越远的点会有更高的概率被选为第n+1个聚类中心。在选取第一个聚类中心（n=1）时同样通过随机的方法。可以说这也符合我们的直觉，聚类中心当然是互相离得越远越好。当选择完初始点后，K-means++后续的执行和经典K均值算法相同，这也是对初始值选择进行改进的方法等共同点。”","text_tokens":["选择","重要","相同","进行","则","受","，","当","选为","未必","第一","中","也","一个","符合","易","这个","初始化","原始","只能","集中","我们","均值","到","一步","可以","了","好","共同点","局部","受到","和","当然","值","<","预先","执行","最","缺点","）","该值","点","如下","开始","第一个","是"," ","因为","分布","当属","收敛","划分","越","人工","更","选取","+","概率","完","点会","就是","（","假设","第","等","吻合","最优","++","初始值","4","大","说","随机","时","初始","真实","数据分布","一部分","经典","”","2","3","被","在","被选为","“","离散","-","思想","n","很","算法","1","预先确定","部分","0","足够","要","噪点","同点","通过","=","后续","有","k","关键","影响","一部","而","方法","。","聚类","效果","点后","个","共同","需要","影响力","这","作为","单一","■","应该","确定","越远","最具","同样","且","按照","解决","改进","数据","类","距离","问题","因此","直觉","高","means","中心","互相","个点","离得","样本","当前","对","已经","很大","的","这类","主要","聚"],"title":"聚类kmeans++","title_tokens":["++","kmeans","聚类"]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E8%81%9A%E7%B1%BB/%E8%81%9A%E7%B1%BB-k%E5%9D%87%E5%80%BC/","text":"K 均值 K 均值是基于样本点间的几何距离来度量聚类的通用目的算法。由于集群围绕在聚类中心，结果会接近于球状并具有相似的大小。 我们之所以推荐该算法给初学者，是因为它不仅足够简单，而且足够灵活，对于大多数问题都能给出合理的结果。 是一个简单的聚类算法，把n的对象根据他们的属性分为k个分割，k< n。算法的核心就是要优化失真函数J,使其收敛到局部最小值但不是全局最小值。 “它的基本思想是，通过迭代方式寻找K个簇（Cluster）的一种划分方案，使得聚类结果对应的代价函数最小。特别地，代价函数可以定义为各个样本距离所属簇中心点的误差平方和” 关于K-Means聚类的文章，参见机器学习算法-K-means聚类。关于K-Means的推导，里面可是有大学问的，蕴含着强大的EM思想。 优点 ： K 均值是最为流行的聚类算法，因为它足够快速、足够简单，如果你的预处理数据和特征工程都做得十分有效，那它将具备令人惊叹的灵活性。 算法简单，容易实现 ； 算法速度很快； 对处理大数据集，该算法是相对可伸缩的和高效率的，因为它的复杂度大约是O(nkt)，其中n是所有对象的数目，k是簇的数目,t是迭代的次数。通常k<<n。这个算法通常局部收敛。“但一般情况下达到的局部最优已经可以满足聚类的需求。” 算法尝试找出使平方误差函数值最小的k个划分。当簇是密集的、球状或团状的，且簇与簇之间区别明显时，聚类效果较好。 缺点 ： 该算法需要指定集群的数量，而 K 值的选择通常都不是那么容易确定的。另外，如果训练数据中的真实集群并不是类球状的，那么 K 均值聚类会得出一些比较差的集群。 对数据类型要求较高，适合数值型数据； 可能收敛到局部最小值，在大规模数据上收敛较慢 分组的数目k是一个输入参数，不合适的k可能返回较差的结果。 对初值的簇心值敏感，对于不同的初始值，可能会导致不同的聚类结果； 不适合于发现非凸面形状的簇，或者大小差别很大的簇。 对于”噪声”和孤立点数据敏感，少量的该类数据能够对平均值产生极大影响。 “受初值和离群点的影响每次的结果不稳定、 结果通常不是全局最优而是局部最优解、 无法很好地解决数据簇分布差别比较大的情况（比如一类是另一类样本数量的100倍）、 不太适用于离散分类等” “样本点只能被划分到单一的类中。” 实现 ： Python - http://scikit-learn.org/stable/modules/clustering.html#k-means R - https://stat.ethz.ch/R-manual/R-devel/library/stats/html/kmeans.html 调优 “（1）数据归一化和离群点处理。 K均值聚类本质上是一种基于欧式距离度量的数据划分方法，均值和方差大的维度将对数据的聚类结果产生决定性的影响，所以未做归一化处理和统一单位的数据是无法直接参与运算和比较的。同时，离群点或者少量的噪声数据就会对均值产生较大的影响，导致中心偏移，因此使用K均值聚类算法之前通常需要对数据做预处理。 （2）合理选择K值。 K值的选择是K均值聚类最大的问题之一，这也是K均值聚类算法的主要缺点。实际上，我们希望能够找到一些可行的办法来弥补这一缺点，或者说找到K值的合理估计方法。但是，K值的选择一般基于经验和多次实验结果。例如采用手肘法，我们可以尝试不同的K值，并将不同K值所对应的损失函数画成折线，横轴为K“的取值，纵轴为误差平方和所定义的损失函数，如图5.3所示。 “（3）采用核函数。” 采用核函数是另一种可以尝试的改进方向。传统的欧式距离度量方式，使得K均值算法本质上假设了各个数据簇的数据具有一样的先验概率，并呈现球形或者高”“维球形分布，这种分布在实际生活中并不常见。面对非凸的数据分布形状时，可能需要引入核函数来优化，这时算法又称为核K均值算法，是核聚类方法的一种[6]。核聚类方法的主要思想是通过一个非线性映射，将输入空间中的数据点映射到高位的特征空间中，并在新的特征空间中进行聚类。非线性映射增加了数据点线性可分的概率，从而在经典的聚类算法失效的情况下，通过引入核函数可以达到更为准确的聚类结果。”","text_tokens":["定义","一些","得出","这种","离群","分组","也","伸缩","其中","折线","：","要求","法","但是","方向","只能","满足","特征","次数","差别","取值","球形","r","团状","另外","em","生活","大小","复杂","形状","定性","）","平方","决定","解","较","对于","蕴含","学习","初值","中心点","问"," ","几何","直接参与","参见","纵轴","需求","大规","对应","使用","实际上","分割","100","集群","多次","另一类","少量","接近","而是","而且","点间","等","使","初始值","大约","参与","误差","时","真实","分为","在","简单","非线性","算法","1","当簇","方式","足够","特别","经验","差","集","具备",",","有效","或者说","流行","有","而","影响","。","nkt","找出","个","传统","区别","单一","令人","速度","失效","多数","方案","确定","维度","估计","不仅","这一","一般","数据","html","最大","基于","距离","蕴含着","采用","优化","横轴","就","means","将","之间","分类","样本","对","已经","敏感","的","learn","参数",")","进行","并","适合","归一化","希望","活性","可行","与","目的","该","发现","数值","规模","型","我们","均值","o","不太","之前","直接","了","缺点","大多数","工程","不","推荐","是","尝试","合适","凸面","机器","统一","最小","稳定","属性","导致","都","t","孤立","全局","快速","类型","stable","每次","各个","适用","概率","实际","6","（","假设","惊叹","经典","2","极大","无法","离散","n","library","高效率","单位","线性","通过","灵活","所属","调优","初学者","为","推导","平均值","这","cluster","代价","且","函数","解决","改进","把","输入","类","平方和","从而","、","通常","manual","高","它","一类","相似","寻找","噪声","里面","准确","文章","主要","j","高位","令人惊叹","受","，","scikit","ethz",".","新","面对","先验概率","运算","做","中","一个","画成","核聚类","于","称为","更为","结果","可能","到","会","围绕","维","好","局部收敛","值","<","大学","映射","决定性","所以","5.3","实现","核","好地解决","因为","收敛","比较","高效","办法","返回","预处理","https","训练","所示","迭代","较慢","学者","是因为","容易","达到","之一","簇心值","很快","其","大",":","数据分布","但","”","kmeans","“","找到","不是","例如","来","能够","最为","方法","能","聚类","如果","通用","效果","需要","可分","或","下","欧式","较大","优点","明显","基本","手肘","不同","你","问题","因此","clustering","中心","ch","最小值","数量","偏移","很大","之所以","那么","给出","python","选择","他们","给","灵活性","方差","同时","/","http","实验","十分","指定","这个","modules","可以","stats","这时","局部","]","倍","和","使得","产生","关于","点","相对","初学","对象","复杂度","可是","(","情况","分布","常见","数据类型","划分","可","数目","合理","处理","具有","引入","上","[","就是","得","另","最优","先验","又","非","stat","初始","如图","失真","呈现","或者","3","被","较差","效率","-","思想","度量","增加","地","根据","要","比如","密集","大规模","球状","所","平均","大多","k","所有","由于","那","devel","本质","一种","org","强大","；","簇","非凸","空间","损失","核心","弥补","#","该类","未","一样","很"],"title":"聚类k均值","title_tokens":["k","均值","聚类"]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E8%81%9A%E7%B1%BB/%E8%81%9A%E7%B1%BB-%E5%88%86%E5%B1%82%E5%B1%82%E6%AC%A1/","text":"分层 / 层次 分层聚类，又名层次聚类，其算法基于以下概念来实现： 1) 每一个集群都从一个数据点开始； 2) 每一个集群都可基于相同的标准进行合并； 3) 重复这一过程，直至你仅剩下一个集群，这就获得了集群的层次结构。 优点 ：层次聚类的最主要优点，是集群不再假定为类球形。此外，它可以很容易扩展到大数据集。 缺点 ：类似于 K 均值，该算法需要选定集群的数量，即算法完成后所要保留的层次。 实现 ： Python - http://scikitlearn.org/stable/modules/clustering.html#hierarchical-clustering R - https://stat.ethz.ch/R-manual/R-devel/library/stats/html/hclust.html","text_tokens":["python","相同","进行","以下","，","ethz",".","/","扩展","http","一个","：","于","modules","该","结构","仅","均值","到","可以","stats","了","获得","球形","r","最","缺点","合并","层次","点","直至","开始","是","实现","保留"," ","重复","都","后","可","即","概念","hierarchical","https","stable","集群","容易","剩下","不再","此外","scikitlearn","选定","其","大","hclust","stat",":","2","3","所要","假定","-","算法","1","library","为类","来","集","标准","k","从","。","又名","聚类","类似","devel","需要","这","层次结构","优点","过程","org","；","这一","数据","你","html","完成","基于","clustering","分层","manual","就","主要","每","它","ch","数量","#","的","很",")"],"title":"聚类分层层次","title_tokens":["层次","分层","聚类"]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E8%81%9A%E7%B1%BB/%E8%81%9A%E7%B1%BB-%E6%94%BE%E5%B0%84%E4%BC%A0%E6%92%AD/","text":"仿射传播 仿射传播是一种相对较新的聚类算法，它基于两个样本点之间的图形距离来确定集群，其结果倾向于更小且大小不等的集群。 优点 ：仿射传播不需要指出明确的集群数量，但需要指定“sample preference”和“damping”等超参数。 缺点 ：仿射传播的主要缺点是训练速度较慢，且需要大量内存，因而难于扩展到大数据集。此外，该算法同样在假定潜在的集群要接近于球状。 实现 ： Python - http://scikit-learn.org/stable/modules/clustering.html#affinity-propagation R - https://cran.r-project.org/web/packages/apcluster/index.html","text_tokens":["packages","python","潜在","，","scikit",".","新","/","扩展","http","cran","指定","：","于","modules","该","index","preference","结果","到","r","和","大小","缺点","damping","超","不","点","相对","较","project","是","实现"," ","传播","指出","大小不等","affinity","https","更","训练","stable","较慢","集群","接近","仿射","web","等","此外","内存","难于","其","大","两个",":","但","”","在","“","假定","-","算法","要","来","集","球状","倾向","。","聚类","不等","需要","因而","apcluster","速度","一种","优点","图形","org","确定","同样","且","sample","数据","html","基于","距离","clustering","之间","小且","它","样本","propagation","数量","#","的","learn","明确","参数","主要","大量"],"title":"聚类放射传播","title_tokens":["传播","放射","聚类"]},{"location":"ml-%E6%97%A0%E7%9B%91%E7%9D%A3/%E8%81%9A%E7%B1%BB/%E8%81%9A%E7%B1%BB/","text":"聚类是基于数据内部结构来寻找样本自然族群（集群）的无监督学习任务，使用案例包括用户画像、电商物品聚类、社交网络分析等。 由于聚类属于无监督学习，也就不会输出“正确的答案”，评价结果时往往要用到数据可视化。如果你需要“正确的答案”，亦即训练集中存在预标注的集群，那么用分类算法会更加合适。 聚类算法是指对一组目标进行分类，属于同一组（亦即一个类，cluster）的目标被划分在一组中，与其他组目标相比，同一组目标更加彼此相似（在某种意义上）。 例子： K-均值（k-Means） k-Medians 算法 Expectation Maximi 封层 ation (EM) 最大期望算法（EM） 分层集群（Hierarchical Clstering） 优点： 让数据变得有意义 缺点： 结果难以解读，针对不寻常的数据组，结果可能无用。","text_tokens":["内部结构","进行","，","针对","让","目标","画像","同","彼此","clstering","与","不会","分析","也","亦","一个","中","可视化","正确","：","往往","集中","结构","某种意义","网络分析","结果","均值","封层","可能","预","会","包括","无","em","内部","缺点","）","族群","不","网络","学习","是","(","物品","社交"," ","组","expectation","合适","medians","难以","划分","期望","即","使用","hierarchical","训练","集群","上","（","更加","等","任务","时","答案","”","被","属于","在","“","-","变得","算法","要","无用","来","意义","一组","案例","其他","评价","k","有","ation","。","由于","存在","聚类","如果","需要","某种","用","电商","cluster","自然","优点","用到","指","数据","你","类","相比","最大","基于","、","分层","就","例子","means","分类","监督","样本","用户","相似","寻找","可视","寻常","对","的","解读","输出","那么","标注","maximi",")"],"title":"聚类","title_tokens":["聚类"]},{"location":"%E5%8C%97%E4%BA%AC/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE/","text":"在机器学习领域，“维度(Dimensionality)”通常指数据集中的特征数量（即输入变量的个数）。 当特征的个数特别大的时候（相对于数据集中观测样本的数量来说），训练出一个有效的模型，对算法要求就会特别高(即，用现有的算法训练出一个有效的模型特别困难)。这就是所谓的“维度灾难(Curse of Dimensionality)”，特别是对依赖于距离计算的聚类算法而言。 对于“维度灾难”，有位 Quora 用户给出了一个非常好的类比： 假设有一条100码的直线，而你在该直线的某处掉了一枚硬币。要找回硬币并不难，你只需沿着这条线走就行了，最多花上你2分钟时间。 ​ 然后，假设有一个长和宽都是100码的广场，而你是把硬币掉在广场的某个位置。现在再想找回它，可就不容易了，这好比在两个并排的足球场中找针，可能会耗上你几天时间。 ​ 再然后，假设是一个长、宽、高都是100码的立方体，那就好比是在30层楼高的大型体育场内找zhen找针…… ​ 随着维度的增加，在空间中搜索的难度也会变得愈加困难。 Quora链接： https://www.quora.com/What-is-the-curse-of-dimensionality/answer/Kevin-Lacker 这就需要数据降维的办法： 特征选取和特征提取。 “机器学习中的数据维数与现实世界的空间维度本同末离。在机器学习中，数据通常需要被表示成向量形式以输入模型进行训练。但众所周知，对向维向量进行处理和分析时，会极大地消耗系统资源，甚至产生维度灾难。因此，进行降维，即用一个低维度的向量表示原始高维度的特征就显得尤为重要。常见的降维方法有主成分分析、线性判别分析、等距映射、局部线性嵌入、拉普拉斯特征映射、局部保留投影等。”","text_tokens":["随着","表示","重要","进行","系统资源","并","，","一枚",".","当","时候","/","甚至","几天","消耗","赖于","降维","与","周知","并排","也","中","向量","一个","分析","广场","：","最多","要求","curse","原始","嵌入","于","quora","集中","该","搜索","尤为重要","想","层楼","尤为","宽","现有","特征","普拉斯","普拉","可能","会","本同末离","足球","现实","某个","了","观测","好","形式","分钟","局部","is","what","和","好比","映射","）","产生","com","不","相对","answer","of","对于","学习","是","现在","(","dimensionality","找回","依赖于"," ","计算","一条","众所周知","保留","机器","个数","有位","常见","都","拉普","可","领域","即","办法","处理","资源","https","训练","直线","位置","选取","100","立方体","依赖","容易","上","类比","这条","不难","就是","（","假设","链接","等","就行了","然后","走","只","the","变量","世界","大","两个","沿着","球场",":","时","但","”","低","2","极大","被","在","掉","“","需","显得","-","即用","系统","非常","增加","算法","变得","www","地","时间","判别","特别","要","找","体育","线性","有效","难度","灾难","维数","大型","有","方法","而","内","来说","。","以","成分","体育场","聚类","…","愈加","那","需要","这","用","等距","硬币","出","所谓","再","拉普拉斯","特征提取","而言","会耗","维度","码","找针","指","把","困难","数据","输入","你","成","判别分析","某处","有主","距离","、","通常","​","因此","就","高","这条线","zhen","空间","投影","kevin","它","足球场","样本","用户","模型","对","数量","30","立方","长","对向维","的","lacker","为重","最多花","提取","拉斯","给出",")"],"title":"维度灾难","title_tokens":["维度","灾难"]}]}